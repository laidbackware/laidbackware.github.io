{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Matt's Hackery","text":"<p>This page is a collection of useful patterns and tutorials I've collected over the years.</p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/","title":"Tanzu Supply Chains Component Architecture","text":"<p>A Component defines the API spec that is passed up to the supply chain, the Tekton pipeline run spec, inputs and outputs.</p> <p>See hello-component for a basic example of a component.</p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/#variables","title":"Variables","text":"<p>If a component requires variable as inputs, their schema is defined under the component spec as shown below.</p> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  config:\n  - path: spec.who-dis\n    schema:\n      type: object\n      properties:\n        name:\n          type: string\n          description: String input to be printed after \"Hello \"\n          example: bob\n      required:\n        - name\n</code></pre> <p>When a supply chain is applied into the cluster all components schemas are aggregated together to form the API of the workload.</p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>Tekton by design does not pass not pass data between workspace spaces and a workspace should be bound to a pipeline run. To get around this Tanzu Supply Chains use the container registry for inter-pipeline storage. If a component needs to store data to pass on to another component it needs to contain a system task that pushes the contents of a workspace to the container registry.</p> <p>The result of the task includes the URL and sha of the image, which is passed back to via the pipelinerun to the stage and the workloadrun. Subsequent components must default the names of any inputs, then within the pipeline define the fetch steps which pull the inputs into the relevant workspace.</p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/#output-example","title":"Output example","text":"<p>This example taken from the <code>carvel-package</code> component, which builds a Carvel package and pushes the manifest to the image repository.</p> <p>The component maps the output to a container registry URL and digest generated by the pipeline.</p> <p><code>component.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  ...\n  outputs:\n    - name: package\n      type: package\n      digest: $(pipeline.results.digest) # optional field. results.digest used by default\n      url: $(pipeline.results.url) # optional field. results.url used by default\n</code></pre></p> <p>In the pipeline the store task expects a workspace called input, which it then pushes to the config repo defined by the workload. The secrets are handled by the standard Tekton mechanism for Docker or the namespace provisioner.</p> <p><code>pipeline.yaml</code> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nspec:\n...\n  tasks:\n    ... # Some logic to populate the workspace `shared-data`\n    - name: store\n      runAfter:\n        - previous-task\n      params:\n        - name: workload-name\n          value: $(params.workload-name)\n      taskRef:\n        name: store-content-oci\n      workspaces:\n        - name: input\n          workspace: shared-data\n  results:\n    - name: url\n      description: url of the resulting source object you can use in your chain\n      type: string\n      value: $(tasks.store.results.url)\n    - name: digest\n      description: digest of the shipped content sent to 'url'\n      type: string\n      value: $(tasks.store.results.digest)\n</code></pre></p> <p>When building a supply chain using the carvel-package component, the outputs become available to all subsequent components in the supply chain.</p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/#input-example","title":"Input example","text":"<p>This example taken from the <code>deployer</code> component, takes a Carvel package as an input and deploys it into the workload namespace.</p> <p>To use the output of a previous component it must be called by name and type in the component definition. The <code>pipelineRun</code> definition then passes in the input as a variable named <code>oci-image-with-yaml</code> into the pipeline.</p> <p><code>component.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  ...\n  inputs:\n    - name: package\n      type: package\n  pipelineRun:\n  ...\n  params:\n    - name: oci-image-with-yaml\n      value: $(inputs.package.url)\n  workspaces:\n      - name: shared-data\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre></p> <p>The pipeline definition takes the <code>oci-image-with-yaml</code> variable from the <code>pipelineRun</code> and passes it down into the task which sets up the workspace.</p> <p>The <code>fetch-tgz-content-oci</code> task is a TAP system task which will pull an OCI image and unpack it into a workspace. As with outputs, the secrets are handled by the standard Tekton mechanism for Docker or the namespace provisioner.</p> <p><code>pipeline.yaml</code> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\n...\nspec:\n  params:\n    - name: oci-image-with-yaml\n      type: string\n    ...\n  workspaces:\n    - name: shared-data\n  tasks:\n    - name: fetch-yaml\n      workspaces:\n        - name: store\n          workspace: shared-data\n      params:\n        - name: url\n          value: $(params.oci-image-with-yaml)\n      taskRef:\n        name: fetch-tgz-content-oci\n    - name: deployer\n      workspaces:\n        - name: content\n          workspace: shared-data\n      ...\n      taskRef:\n        name: deployer\n      runAfter:\n        - fetch-yaml\n</code></pre></p>"},{"location":"app-platform/tanzu-supply-chains/component-architecture/#pipelinerun","title":"PipelineRun","text":"<p>This section of the API partially replicates the Tekton PipelineRun API.</p> <p>The example below taken from the <code>deloyer</code> OOB component takes variables from the input, calls the <code>deployer</code> pipeline by name, sets the UIDs and GIDs for the tasks and sets the workspace to use a dynamically created persistent volume.</p> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\n  pipelineRun:\n    params:\n      - name: oci-image-with-yaml\n        value: $(inputs.package.url)\n      ...\n    pipelineRef:\n      name: deployer\n    taskRunTemplate:\n      podTemplate:\n        securityContext:\n          fsGroup: 1000\n          runAsGroup: 1000\n          runAsUser: 1001\n    workspaces:\n      - name: shared-data\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre> <p>When the stage is created it will create a PipelineRun from the template above.</p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/","title":"Tanzu Supply Chains Component Authoring","text":"<p>Engineering a component requires thinking in layers. </p> <ul> <li>At the bottom most layer you have the step within the Tekton task, which is a shell script. It's recommended to write the script in such a way that it can be executed and tested in isolation without the extra overhead of Tekton. </li> <li>Once the script is running then the Tekton wrapper can be place around it and it can be tested with a pipeline run.</li> <li>Once the pipeline run completes successfully it can be wrapped in a component.</li> </ul> <p>The instructions below detail the steps needed to build up these layers to create a simple stateful set based upon the pod spec output of the conventions server.</p> <p>Warning this tutorial uses ytt overlays, so you may want to check out some tutorials on that before continuing.</p> <p>This example uses simple shell scripts tested on Linux.</p> <p>The raw yaml and scripts below can be found here.</p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>The component will expect an input of the pod spec, which is an output from the convention server. An input from the convention server will contain a single file in the root of the workspace called <code>app-config.yaml</code>. Below is an abbreviated pod spec, with the 2 sections that will be reused.</p> <pre><code>template:\n  spec:\n    containers:\n    - env:\n      - name: JAVA_TOOL_OPTIONS\n        value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.health.probes.enabled=\"true\" -Dserver.port=\"8080\" -Dserver.shutdown.grace-period=\"24s\"\n      image: harbor.lab:8443/tap-workload/friday-workload@sha256:cbd7c9d033f3b4a3ed5faf23a02e69bcfc9443ab405c49d9433d9af656b1eedd\n...\n</code></pre> <p>To match the existing workload types and allow it to be used by the sebsequent existing supply chain components, 2 outputs will be created. <code>oci-yaml-files</code> containing the base yaml for the package and <code>oci-ytt-files</code> which will contain the variables.</p> <p>The files listed below will be expected by the <code>carvel-package</code> component.</p> <pre><code>/workspace/oci-yaml-files\n|-- appconfig.yaml # contains the base spec\n\n/workspace/oci-ytt-files\n|-- web-template-overlays.yaml # contains the overlays needed to replace values in appconfig.yaml\n|-- web-template-values.yaml # contains the base values\n</code></pre> <p>The component will also take a single variable input of the workload name.</p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/#writing-the-script","title":"Writing the script","text":"<p>Because the script will require a file structure to be layed out, a wrapper script is needed to first setup a simulated Tekton workspace with a dummy <code>appconfig.yaml</code> to replicate a convention component output.</p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/#test-wrapper","title":"Test wrapper","text":"<p>The outer test wrapper script needs to setup any necessary files and export all environment variables used by the task script.</p> <p>File <code>app-config.yaml</code> is an abbreviated version of a convention component output containing the fields needed to build a K8s resource.</p> <p>Because the component has 2 outputs, 2 directories need to be passed into the script. <code>WORKSPACE_YAML</code> will be used for the <code>oci-yaml-files</code> output and <code>WORKSPACE_YTT</code> for the <code>oci-ytt-files</code> output.</p> <p><code>test-task-script.sh</code> <pre><code>#!/bin/bash\n\nset -eux\n\nreadonly SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\nreadonly TEMP_DIR_YAML=\"$(mktemp -d)\"\nreadonly TEMP_DIR_YTT=\"$(mktemp -d)\"\ntrap 'rm -rf -- \"$TEMP_DIR_YAML\" &amp;&amp; rm -rf -- \"$TEMP_DIR_YTT\"' EXIT\n\ncat &lt;&lt;EOT &gt;&gt; ${TEMP_DIR_YAML}/app-config.yaml\ntemplate:\n  spec:\n    containers:\n    - env:\n      - name: JAVA_TOOL_OPTIONS\n        value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.health.probes.enabled=\"true\" -Dserver.port=\"8080\" -Dserver.shutdown.grace-period=\"24s\"\n      image: harbor.lab:8443/tap-workload/friday-workload@sha256:cbd7c9d033f3b4a3ed5faf23a02e69bcfc9443ab405c49d9433d9af656b1eedd\nEOT\n\nexport WORKSPACE_YAML=$TEMP_DIR_YAML\nexport WORKSPACE_YTT=$TEMP_DIR_YTT\nexport WORKLOAD_NAME=\"dummy\"\n\n${SCRIPT_DIR}/task-script.sh\n</code></pre></p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/#task-script","title":"Task script","text":"<p>The task script expects 3 environment variables, which were set by the wrapper. <code>WORKSPACE_YAML</code> and <code>WORKSPACE_YTT</code> represent the workspace paths and <code>WORKLOAD_NAME</code> is needed to substitute into the yaml. By make these environment variables it means that the script can be Tekton agnostic and enables running/testing outside Tekton.</p> <p><code>web-template-values.yaml</code> and <code>web-template-overlays.yaml</code> are rendered using Bash to inject the workload name.</p> <p>The output <code>appconfig.yaml</code> needs to be created with ytt because it will be referencing the values from the data source from the input <code>app-config.yaml</code>.</p> <p><code>task-script.sh</code> <pre><code>#!/bin/bash\n\nset -euxo pipefail\n\nTEMP_DIR=\"$(mktemp -d)\"\n\n# clean the workspace directory by moving any inputs to the temp directory\nmv $WORKSPACE_YAML/* $TEMP_DIR/\n\ncat &lt;&lt;EOT &gt;&gt; ${WORKSPACE_YTT}/web-template-values.yaml\n#@data/values-schema\n---\n#@schema/desc \"Used to generate resource names.\"\n#@schema/example \"tanzu-java-web-app\"\n#@schema/validation min_len=1\nworkload_name: \"${WORKLOAD_NAME:?}\"\n\n#@schema/desc \"Number of repicas.\"\nreplicas: 1\nEOT\n\ncat &lt;&lt;EOT &gt;&gt; ${WORKSPACE_YTT}/web-template-overlays.yaml\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n#@ load(\"@ytt:template\", \"template\")\n\n#@overlay/match by=overlay.subset({\"apiVersion\":\"apps/v1\", \"kind\": \"StatefulSet\"})\n---\nspec:\n  #@ if data.values.env:\n  #@overlay/match missing_ok=True\n  #@overlay/replace or_add=True\n  replicas: #@ data.values.replicas\n  #@ end\nEOT\n\nOUTPUT_APPCONFIG=`cat &lt;&lt;EOF\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: \"${WORKLOAD_NAME:?}\"\nspec:\n  selector:\n    matchLabels:\n      app: \"${WORKLOAD_NAME:?}\"\n  serviceName: \"${WORKLOAD_NAME:?}\"\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: \"${WORKLOAD_NAME:?}\"\n    spec:\n      containers:\n      - name: \"${WORKLOAD_NAME:?}\"\n        env: #@ data.values.template.spec.containers[0].env\n        image: #@ data.values.template.spec.containers[0].image\n        ports:\n        - containerPort: 80\n          name: web\nEOF\n`\n\necho \"$OUTPUT_APPCONFIG\" | ytt -f - --data-values-file ${TEMP_DIR}/app-config.yaml &gt; ${WORKSPACE_YAML}/appconfig.yaml\n\nls -l ${WORKSPACE_YAML}\nls -l ${WORKSPACE_YTT}\n</code></pre></p>"},{"location":"app-platform/tanzu-supply-chains/component-authoring/#injecting-the-script-into-the-task","title":"Injecting the script into the task","text":"<p>Once the script is working exactly as expected it can be injected to the task spec.</p> <p>For this script to work the kube context be pointing to a TAP 1.9+ cluster with Tanzu Supply Chains installed.</p> <p>The script below generates an overlay to inject the variables, gets an image ref from the running cluster that container the necessary dependencies, strips the script from <code>task.yaml</code> to remove extra overlays and re-creates <code>task.yaml</code> with the injected values.</p> <pre><code>#!/bin/bash\n\nset -eux\n\nreadonly SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\n\nreadonly TEMP_DIR=\"$(mktemp -d)\"\ntrap 'rm -rf -- \"$TEMP_DIR\"' EXIT\n\n# cp ${SCRIPT_DIR}/task.yaml-template $TEMP_DIR/task.yaml\n\nTASK_IMAGE=$(kubectl get task -n alm-catalog deployer -o jsonpath='{.spec.steps[0].image}')\n\nOVERLAY=`cat &lt;&lt;EOF &gt;&gt; ${TEMP_DIR}/overlay.yaml\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n\n#@overlay/match by=overlay.subset({\"apiVersion\":\"tekton.dev/v1\", \"kind\": \"Task\"})\n---\nspec:\n  steps:\n    #@overlay/match by=overlay.index(0)\n    - image: #@ data.values.task_image\n      script: #@ data.values.script\nEOF\n`\n\n# Strip script to remove inline overlays\nsed -n -e '/  script:/{' -e 'p' -e ':a' -e 'N' -e '/  stepTemplate:/!ba' -e 's/.*\\n//' -e '}' \\\n  -e 'p' ${SCRIPT_DIR}/task.yaml &gt; ${TEMP_DIR}/task.yaml\n\nytt -f ${TEMP_DIR}/task.yaml -f ${TEMP_DIR}/overlay.yaml \\\n  --data-value-file script=${SCRIPT_DIR}/task-script.sh \\\n  --data-value task_image=${TASK_IMAGE} &gt; ${SCRIPT_DIR}/task.yaml\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/hello-component/","title":"Tanzu Supply Chains Minimal Component Example","text":"<p>The example below is a hello world component that does not have any input/outputs, only a single variable to print inside the container.</p> <p>The raw yaml can be found here.</p>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#component-definition","title":"Component definition","text":"<p>The variable <code>who-dis</code> is passed from the workload into the <code>Component</code>, to the Pipeline and finally the <code>Task</code>. At execution time the WorkloadRun will create a <code>Stage</code> for each <code>Component</code>. The <code>Stage</code> will create <code>Resumptions</code> and <code>PipelineRuns</code>, with the <code>PipelineRun</code> creating <code>TaskRuns</code>.</p> <p>Whilst the <code>Task</code> could be written inline inside the pipeline, externalising it enables testing of the <code>Task</code> separately if needed.</p> <p><code>component.yaml</code> <pre><code>---\napiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\nmetadata:\n  labels:\n    supply-chain.apps.tanzu.vmware.com/catalog: hello-component\n  name: hello-0.0.1\nspec:\n  config:\n  - path: spec.who-dis\n    schema:\n      type: object\n      properties:\n        name:\n          type: string\n          description: String input to be printed after \"Hello \"\n          example: bob\n      required:\n        - name\n  description: Outputs hello &lt;who-dis&gt;\n  pipelineRun:\n    params:\n    - name: who-dis\n      value: $(workload.spec.who-dis.name)\n    pipelineRef:\n      name: hello-pipeline\n    taskRunTemplate:\n      podTemplate:\n        securityContext:\n          fsGroup: 1000\n          runAsGroup: 1000\n          runAsUser: 1001\n---\napiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: hello-pipeline\nspec:\n  params:\n  - description: Input string\n    name: who-dis\n    type: string\n  tasks:\n    - name: hello-there\n      params:\n      - name: who-dis\n        value: $(params.who-dis)\n      taskRef:\n        name: hello-task\n---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: hello-task\nspec:\n  params:\n  - name: who-dis\n    type: string\n  steps:\n    - name: echo\n      image: alpine\n      script: |\n        #!/bin/sh\n        set -e\n        echo \"Hello $(params.who-dis)\"\n</code></pre> Apply the component.</p> <pre><code>kubectl apply -f component.yaml\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#supply-chain-spec","title":"Supply Chain Spec","text":"<p>The following yaml creates a supply chain against the component.</p> <p><code>supply-chain.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: SupplyChain\nmetadata:\n  name: hello.example.tanzu-0.0.1\nspec:\n  defines: # Describes the workload\n    kind: HelloApp\n    plural: helloapps\n    group: example.tanzu\n    version: v1alpha1\n  stages: # Describes the stages\n    - name: hello\n      componentRef: # References the components\n        name: hello-0.0.1\n</code></pre></p> <p>Create and check the supply chain is ready.</p> <pre><code>kubectl apply -f supply-chain.yaml\nkubectl get supplychain hello.example.tanzu-0.0.1\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#workload-spec","title":"Workload Spec","text":"<p>The spec of a workload can be generated using the following command.</p> <pre><code>tanzu workload generate &lt;planned worklaod name&gt; -k &lt;spec.defines.plural&gt;.&lt;spec.defines.group&gt;\ntanzu workload generate hello-fred -k helloapps.example.tanzu\n</code></pre> <p>The following yaml creates a workload against the supply chain:</p> <p><code>workload.yaml</code> <pre><code>apiVersion: example.tanzu/v1alpha1\nkind: HelloApp\nmetadata:\n  name: hello-fred\nspec:\n  who-dis:\n    name: fred\n</code></pre></p>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#running-the-workload","title":"Running the workload","text":"<p>This example will deploy everything into the same namespace for simplicity. Under normal operations the supply chain and workload would be deployed into separate namespaces.</p> <p>Apply the workload.</p> <pre><code>tanzu workload create -f workload.yaml\n</code></pre> <p>Query its state.</p> <pre><code>tanzu workload get hello-fred\n</code></pre> <p>This will automatically trigger a workload run which can be monitored with the following commands. If the supply chain has inputs/outputs they can be queried here under <code>status.stages</code> using <code>kubectl</code>.</p> <pre><code>tanzu workload run get $(kubectl get helloappruns.example.tanzu \\\n  -o jsonpath='{.items[0].metadata.name}')\n\nkubectl describe helloappruns.example.tanzu\n\nkubectl get helloappruns.example.tanzu -o yaml\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#debugging","title":"Debugging","text":"<p>From TAP 1.9 the components will run in the supply chain namespace, which will be separate from the workload namespace. Under normal operation this will prevent the end user from querying Tekton resources and <code>pods</code> of the <code>stages</code> in the supply chain namespace for security reasons. If the end user needs to see workload run logs they can use the following command:</p> <pre><code>tanzu workload logs hello-fred\n</code></pre> <p>The output of the command can be limited using the following syntax:</p> <pre><code>tanzu workload logs NAME --since 1h\ntanzu workload logs NAME --since 1h --namespace default --run runname\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/hello-component/#advanced-debugging","title":"Advanced debugging","text":"<p>With access to the supply chain namespace the following commands are available.</p> <pre><code>kubectl get stage\nkubectl tree stage &lt;stage-name&gt;\nkubectl get/describe pipeline\nkubectl get/describe pipelinerun\nkubectl get/describe task\nkubectl get/describe taskrun\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/private-ca-hack/","title":"Hack Tanzu Supply Chain Tasks to Allow Private Registry","text":"<p>All tasks must be copied to the supply chain names space ahead of time for this procedure to work.</p> <p>Export the supply chain namespace name:</p> <pre><code>export NS_CHAIN=\"my-supply-chain\"\n</code></pre> <p>Run the following to string substitute in the skip TLS flags. It will get all tasks inside the supply chain namespace, mutate the commands to skip TLS and apply the result back into the supply chain namespace.</p> <pre><code>kubectl get tasks -n \"${NS_CHAIN:?}\" -o yaml | \\\n  sed 's/push -i/push --registry-verify-certs=false -i/' | \\\n  sed 's/imgpkg_params -b/imgpkg_params  --registry-verify-certs=false -b/' | \\\n  sed 's/pull -i/pull --registry-verify-certs=false -i/' | \\\n  sed 's/krane config \"/krane config --insecure \"/' | \\\n  kubectl apply -n \"${NS_CHAIN:?}\" -f -\n</code></pre>"},{"location":"app-platform/tanzu-supply-chains/supply-chain-architecture/","title":"Tanzu Supply Chains Component Architecture","text":"<p>TAP 1.8 introduced Tanzu Supply Chains as the next gen replacement for cartographer.</p> <p>In summary Tanzu Supply Chains are a wrapped for Tekton, which allow platform and DevOps engineers to easily create APIs for develops to create their workloads.</p> <p>In it's simplest form a platform engineer can create a supplychain with ~20 lines of yaml using the out of the box (OOB) supply chain components. See the tutorial to generate a supply chain using the CLI.</p> <p>If functionality is needed beyond the OOB components, then you will need to create a custom component.</p>"},{"location":"app-platform/tanzu-supply-chains/supply-chain-architecture/#declarative-api-and-versioning","title":"Declarative API and Versioning","text":"<p>The main difference in user experience when compared to Cartographer is that platform engineers expose a fixed API for each workload type based on the components in the supply chain. As a developer I have no way to modify the supply chain beyond what is provided, like what was possible with <code>ytt</code> overlays in Cartographer.</p> <p>Each workload is versioned using standard K8s resource versioning, meaning that a platform engineer can release newer versions with API breaking changes without impacting existing workloads and give app owners the ability to move between versions.</p> <p>See the Component Architecture doc for how variables are managed.</p>"},{"location":"app-platform/tanzu-supply-chains/supply-chain-architecture/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Components are designed to be single function and modular, meaning that as the catalogue grows, components can easily be plugged together. For example the the only component that pushes configuration to Git is the <code>git-writer-1.0.0</code> component. This is so that the logic exists once.</p>"},{"location":"app-platform/tanzu-supply-chains/supply-chain-architecture/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>See the Component Architecture doc for a detailed description.</p>"},{"location":"k8s/apps/minimal-lb/","title":"Minimal Kubernetes web app + LB","text":"<p>The following yaml will deploy a basic web server and expose it via a load balancer service.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  labels:\n    app: test-pod\nspec:\n  containers:\n  - name: test-webserver\n    image: k8s.gcr.io/test-webserver:latest\n    ports:\n    - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-svc\nspec:\n  selector:\n    app: test-pod\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"k8s/tkgs/admin-access/","title":"How to access the Tanzu with vSphere Products","text":""},{"location":"k8s/tkgs/admin-access/#ssh-to-the-supervisor-cluster-nodes","title":"SSH to the Supervisor Cluster nodes","text":"<p>SSH to the vCenter as root and run to get supervisor creds <code>/usr/lib/vmware-wcp/decryptK8Pwd.py</code>.</p> <p>So long as firewall rules allow, the credentials can be saved and the connection made to the supervisor without access to the vCenter.</p>"},{"location":"k8s/tkgs/admin-access/#ssh-to-the-guest-cluster-nodes","title":"SSH to the Guest Cluster nodes","text":"<p>Commands should be run on the supervisor cluster.</p> <p>SSH commands can be run from anywhere with access to the VMs.</p>"},{"location":"k8s/tkgs/admin-access/#using-private-keys","title":"Using private keys","text":"<p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh -o jsonpath={.data.\"ssh-privatekey\"}  | base64 -d | tee -a privatekey.key\nchmod 600 /tmp/privatekey.key\nssh -i /tmp/privatekey.key vmware-system-user@&lt;guest_IP&gt;\n</code></pre> The private key can be exported and if not should be deleted when finished.</p>"},{"location":"k8s/tkgs/admin-access/#using-the-password","title":"Using the password","text":"<pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh-password -o jsonpath={.data.\"ssh-passwordkey\"} | base64 -d\nssh vmware-system-user@&lt;guest_IP&gt;\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#running-commands-without-kubectl-for-debugging","title":"Running commands without kubectl, for debugging","text":"<p><code>crictl</code> mostly mirrors <code>docker</code></p> <pre><code>sudo -i\ncrictl ps # show running containers\ncrictl logs &lt;container ID&gt; # show container logs\ncrtictl exec pod &lt;container ID&gt; /bin/sh # exec into a container\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#getting-kubectl-inside-a-workload-cluster-control-plane-node","title":"Getting kubectl inside a workload cluster control plane node","text":"<pre><code>sudo -i\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#legacy-tkgs-harbor-admin-ui-access","title":"Legacy TKGS Harbor Admin UI Access","text":"<p><code>kubectl</code> commands must be run from inside a supervisor cluster VM.</p> <pre><code>HARBOR_NAMESPACE=$(kubectl get ns | grep registry- | awk '{print $1}')\nHARBOR_POD_ID=$(echo $HARBOR_NAMESPACE | sed 's/.*-//')\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminUsername}' |base64 -d |base64 -d\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminPassword}' |base64 -d |base64 -d\n</code></pre>"},{"location":"k8s/tkgs/repave-on-7/","title":"Re-pave a vSphere 7 cluster","text":"<p>This patch will cause all nodes to be created on a vSphere 7 workload cluster.</p> <p>Commands must be run against the supervisor.</p>"},{"location":"k8s/tkgs/repave-on-7/#find-the-machinedeployments-object-for-your-cluster","title":"Find the <code>machinedeployments</code> object for your cluster","text":"<pre><code>kubectl get machinedeployments.cluster.x-k8s.io -n &lt;namespace&gt;\n</code></pre> <pre><code>kubectl patch machinedeployments.cluster.x-k8s.io ${TEMPLATE_NAME:?} --type merge -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\"`date +'%s'`\\\"}}}}}\" -n &lt;namespace&gt;\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/","title":"Set default PSA on vSphere with Tanzu 1.26+ clusters","text":"<p>From TKR 1.26 and above vSphere with Tanzu changed the default Pod Security Admission (PSA) level to enforced, meaning that each namespace must be labelled to relax the policy. The instructions on this page explain how to create a custom cluster class from the default class which sets the default PSA level to audit.</p>"},{"location":"k8s/tkgs/set-default-psa/#warning","title":"Warning","text":"<p>Taken from the vSphere with Tanzu docs.</p> <p>\"Custom ClusterClass is an experimental Kubernetes feature per the upstream Cluster API documentation. Due to the range of customizations available with custom ClusterClass, VMware cannot test or validate all possible customizations. Customers are responsible for testing, validating, and troubleshooting their custom ClusterClass clusters. Customers can open support tickets regarding their custom ClusterClass clusters, however, VMware support is limited to a best effort basis only and cannot guarantee resolution to every issue opened for custom ClusterClass clusters. Customers should be aware of these risks before deploying custom ClusterClass clusters in production environments.\"</p> <p>Given the statement above and the fact that a cluster cannot currently be switched between cluster classes, it is not recommended to use custom cluster classes.</p> <p>The procedure is based on the {vSphere docs](https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-with-tanzu-tkg/GUID-EFE7DB40-8748-42B5-9694-DBC21F9FB76A.html), which you should always reference to check for changes.</p>"},{"location":"k8s/tkgs/set-default-psa/#procedure","title":"Procedure","text":""},{"location":"k8s/tkgs/set-default-psa/#step-1-copy-the-default-clusterclass","title":"Step 1 - Copy the default ClusterClass","text":"<p>Export the variables to match your environment.</p> <pre><code>export NS=\"ns1\"\nexport CCC_NAME=\"my-cc\"\n</code></pre> <p>Export the default ClusterClass, strip unnecessary fields and update the name. <pre><code>kubectl -n $NS get clusterclass tanzukubernetescluster -o yaml &gt; ccc.yaml\nsed -i '/creationTimestamp:/d' ccc.yaml &amp;&amp; sed -i '/generation:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml &amp;&amp; sed -i '/uid:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml\nsed -i \"s/  name: tanzukubernetescluster/  name: ${CCC_NAME}/g\" ccc.yaml\n</code></pre></p>"},{"location":"k8s/tkgs/set-default-psa/#step-2-modify-the-custom-clusterclass","title":"Step 2 - Modify the custom ClusterClass","text":"<p>It's recommended to manually edit the file to set policy, but automated step are listed below.</p> <ul> <li>Open ccc.yaml in your favourity editor.</li> <li>Search for <code>controlPlaneFilesAdmissionConfigurationk8s126</code> and scroll up to see the <code>AdmissionConfiguration</code> template.</li> <li>Modify the yaml to set your policy by updating the section <code>plugins.0.configuration.defaults</code>. Scrolling up 30 lines will show the K8s 1.25 policy which does not enforce.</li> </ul>"},{"location":"k8s/tkgs/set-default-psa/#automated-steps-to-replace-out-the-fields-of-the-k8s-125-policy","title":"Automated steps to replace out the fields of the K8s 1.25 policy.","text":"<p>Use with caution as this was only tested with 1.26.</p> <pre><code>sed -i -E 's/enforce: \"restricted\"/warn: \"restricted\"\\n                      warn-version: \"latest\"/' ccc.yaml\nsed -i -E 's/enforce-version: \"latest\"/audit: \"restricted\"\\n                      audit-version: \"latest\"/' ccc.yaml\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/#step-3-apply-the-custom-clusterclass-to-any-namespace","title":"Step 3 - Apply the custom ClusterClass to any namespace","text":"<p>The ClusterClass to any namespaces where it is needed.</p> <pre><code>export TARGET_NS=\"ns2\"\nsed -i \"s/namespace: .*/namespace: ${TARGET_NS}/g\" ccc.yaml\nkubectl apply -f ccc.yaml\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/#step-4-create-clusters-using-the-custom-clusterclass","title":"Step 4 - Create clusters using the custom ClusterClass","text":"<p>Add the following section to your ClusterClass yamls <pre><code>spec:\n  topology:\n    class: &lt;custom cluster class name&gt;\n</code></pre></p>"},{"location":"k8s/tkgs/troubleshooting/","title":"Bootstrap troubleshooting","text":"<p>SSH onto s supervisor node and <code>sudo -i</code>.</p> <p>Check containers with:</p> <pre><code>crictl ps\n</code></pre> <p>Check if the api server is available.</p> <pre><code>kubectl get pod -A\n</code></pre> <p>Check the state of services and the state of cloud init:</p> <pre><code>systemctl --type service\n\nless /var/log/cloud-init-output.log\njournalctl -xeu  cloud-final\njournalctl -xeu  cloud-init\njournalctl -xeu  cloud-config\njournalctl -xeu  cloud-init-local\n</code></pre> <p>Check logs on all the nodes with:</p> <pre><code>grep -R -i stderr /var/log/pods/*\ngrep -R -i error /var/log/pods/*\ngrep -R -i fail /var/log/pods/*\n</code></pre> <p>Check Kubelet</p> <pre><code>systemctl status kubelet  --no-pager --full\njournalctl -xeun kubelet\ncrictl images ls\n</code></pre>"},{"location":"k8s/tkgs/update-cp-disks/","title":"How to update the control plane disks of a workload cluster on vSphere with Tanzu","text":"<p>The docs explains how to expand the disk of a vSphere with Tanzu control plane VMs in place.</p> <p>THIS PROCEDURE IS NOT SUPPORTED BY VMWARE BY BROADCOM AND MUST BE USED WITH CAUTION! THE AUTHOR CANNOT BE HELD RESPONSIBLE FOR ANY ISSUES.</p> <p>Running without the update validation webhook is risky as no control plane update actions will be validated, so should be done for the minimum amount of time..</p>"},{"location":"k8s/tkgs/update-cp-disks/#connect-to-a-supervisor-vm","title":"Connect to a Supervisor VM","text":"<p>SSH into the vCenter as root and enter a shell.</p> <p>Run: <pre><code>/usr/lib/vmware-wcp/decryptK8Pwd.py\n</code></pre></p> <p>SSH into the IP address provided by the script output as the root user, using the password provided.</p>"},{"location":"k8s/tkgs/update-cp-disks/#on-a-supervisor-vm","title":"On a supervisor VM","text":"<p>Run: <pre><code>kubectl get ValidatingWebhookConfiguration capi-kubeadm-control-plane-validating-webhook-configuration -o yaml &gt; cp-hook-org.yml\ncp cp-hook-org.yml cp-hook-upd.yml\nkubectl get ValidatingWebhookConfiguration vmware-system-tkg-validating-webhook-configuration -o yaml &gt; tkg-hook-org.yml\ncp tkg-hook-org.yml tkg-hook-upd.yml\n</code></pre></p> <p>Edit <code>cp-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: validation.kubeadmcontrolplane.controlplane.cluster.x-k8s.io</code> (this was the first webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Edit <code>tkg-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: default.validating.tanzukubernetescluster.run.tanzu.vmware.com</code> (this was the last webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Run: <pre><code>kubectl apply -f cp-hook-upd.yml\nkubectl apply -f tkg-hook-upd.yml\n</code></pre></p>"},{"location":"k8s/tkgs/update-cp-disks/#updating-workload-cluster-via-supervisor-manifest","title":"Updating workload cluster via Supervisor manifest","text":"<p>On a standard linux machine with a normal connection to the supervisor</p> <p>Now it is possible to update the control plane disks. Be careful to not make any other changes.</p>"},{"location":"k8s/tkgs/update-cp-disks/#updating-class-based-workload-clusers-via-tmc","title":"Updating Class based workload clusers via TMC","text":"<p>Instructions use the Tanzu CLI.</p> <p>Export the existing cluster config to yaml: <pre><code>tanzu management-cluster cluster get &lt;cluster_name&gt; -p &lt;supervisor_namesapce&gt; -m &lt;tmc_supervisor_name&gt; -o yaml &gt; cluster_spec.yaml\n</code></pre></p> <p>Remove the \"meta\" and \"status\" sections.</p> <p>Under spec.topology.variables add the following (update the mount, storageClass and sizes as necessary): <pre><code>spec:\n  topology:\n    variables:\n    - name: controlPlaneVolumes\n      value:\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/containerd\n        name: containerd\n        storageClass: tkgs-storage-policy\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/kubelet\n        name: kubelet\n        storageClass: tkgs-storage-policy\n</code></pre></p>"},{"location":"k8s/tkgs/update-cp-disks/#revert-on-completion","title":"Revert on completion!!!!","text":"<p>Once finished, revert the changes on the supervisor by addding the <code>- UPDATE</code> lines that were removed at the start.</p> <p>You can re-apply the original files by SSHing back to the same supervisor node, which should happen automatically following the connection procedure. </p> <p>Run: <pre><code>kubectl apply -f cp-hook-org.yml\nkubectl apply -f tkg-hook-org.yml\n</code></pre></p> <p>If for any reason you are not able to access the original yaml files you can follow the process in reverse by adding the relevant lines.</p>"},{"location":"shell/bash-recipes/","title":"Bash Recipes","text":""},{"location":"shell/bash-recipes/#run-command-with-stdin-as-an-input","title":"Run command with stdin as an input","text":"<p>Kubbectl apply from an inline yaml string.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: tap-install\nEOF\n</code></pre>"},{"location":"shell/bash-recipes/#conditional-run-command","title":"Conditional run command","text":"<p>Run command based on output of another command. </p> <pre><code>set +e\nnetworks=\"$(docker network inspect kind-tilt 2&gt;&amp;1)\"\nset -e\nif [[ \"$networks\" == *\"network kind-tilt not found\"* ]]; then\n  docker network create --gateway=\"100.127.0.1\" --ip-range=\"100.127.0.0/24\" --subnet=\"100.127.0.0/16\" kind-tilt\nelse\n  echo \"kind-tilt network already exists\"\nfi\n</code></pre>"},{"location":"shell/bash-recipes/#sed","title":"Sed","text":""},{"location":"shell/bash-recipes/#get-all-on-line-text-after-a-string","title":"Get all on line text after a string","text":"<p>E.g. return value after the first occurrence of <code>name:</code> from a file on Linux.</p> <pre><code>sed -n -e '/name/ {s/.*: *//p;q}' /$HOME/.kube/config\n</code></pre>"},{"location":"shell/bash-recipes/#remove-all-lines-starting-with-string","title":"Remove all lines starting with string","text":"<p>E.g. remove all lines starting with \"#@\".</p> <pre><code>cat blah.txt | grep -v \"#~\"\n</code></pre>"},{"location":"shell/bash-recipes/#check-if-a-url-is-available","title":"Check if a URL is available","text":"<p>E.g. check if google.com is contactable. Can be used to check if VPN is connected.</p> <pre><code>status_code=\"$(curl --write-out %{http_code} --silent --output -k /dev/null https://www.google.com)\"\n\nif [[ \"$status_code\" -ne 200 ]] ; then\n  echo \"You are not connected to the VPN. Please connect and re-run this script\"\nelse\n  exit 1\nfi\n</code></pre>"},{"location":"shell/bash-recipes/#check-if-command-is-in-path","title":"Check if command is in path","text":"<p>E.g. check if ytt is available in the shell.</p> <pre><code>if ! [ -x \"$(command -v ytt)\" ]; then\n  echo -e 'ytt CLI not in path.\\nSee https://github.com/carvel-dev/ytt' &gt;&amp;2\n  exit 1\nfi\n</code></pre>"}]}