{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Matt's Hackery","text":"<p>This page is a collection of useful patterns and tutorials I've collected over the years.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/","title":"Tanzu Supply Chains Component Architecture","text":"<p>A Component defines the API spec that is passed up to the supply chain, the Tekton pipeline run spec, inputs and outputs.</p> <p>See hello-component for a basic example of a component.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/#variables","title":"Variables","text":"<p>If a component requires variable as inputs, their schema is defined under the component spec as shown below.</p> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  config:\n  - path: spec.who-dis\n    schema:\n      type: object\n      properties:\n        name:\n          type: string\n          description: String input to be printed after \"Hello \"\n          example: bob\n      required:\n        - name\n</code></pre> <p>When a supply chain is applied into the cluster all components schemas are aggregated together to form the API of the workload.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>Tekton by design does not pass not pass data between workspace spaces and a workspace should be bound to a pipeline run. To get around this Tanzu Supply Chains use the container registry for inter-pipeline storage. If a component needs to store data to pass on to another component it needs to contain a system task that pushes the contents of a workspace to the container registry.</p> <p>The result of the task includes the URL and sha of the image, which is passed back to via the pipelinerun to the stage and the workloadrun. Subsequent components must default the names of any inputs, then within the pipeline define the fetch steps which pull the inputs into the relevant workspace.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/#output-example","title":"Output example","text":"<p>This example taken from the <code>carvel-package</code> component, which builds a Carvel package and pushes the manifest to the image repository.</p> <p>The component maps the output to a container registry URL and digest generated by the pipeline.</p> <p><code>component.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  ...\n  outputs:\n    - name: package\n      type: package\n      digest: $(pipeline.results.digest) # optional field. results.digest used by default\n      url: $(pipeline.results.url) # optional field. results.url used by default\n</code></pre></p> <p>In the pipeline the store task expects a workspace called input, which it then pushes to the config repo defined by the workload. The secrets are handled by the standard Tekton mechanism for Docker or the namespace provisioner.</p> <p><code>pipeline.yaml</code> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nspec:\n...\n  tasks:\n    ... # Some logic to populate the workspace `shared-data`\n    - name: store\n      runAfter:\n        - previous-task\n      params:\n        - name: workload-name\n          value: $(params.workload-name)\n      taskRef:\n        name: store-content-oci\n      workspaces:\n        - name: input\n          workspace: shared-data\n  results:\n    - name: url\n      description: url of the resulting source object you can use in your chain\n      type: string\n      value: $(tasks.store.results.url)\n    - name: digest\n      description: digest of the shipped content sent to 'url'\n      type: string\n      value: $(tasks.store.results.digest)\n</code></pre></p> <p>When building a supply chain using the carvel-package component, the outputs become available to all subsequent components in the supply chain.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/#input-example","title":"Input example","text":"<p>This example taken from the <code>deployer</code> component, takes a Carvel package as an input and deploys it into the workload namespace.</p> <p>To use the output of a previous component it must be called by name and type in the component definition. The <code>pipelineRun</code> definition then passes in the input as a variable named <code>oci-image-with-yaml</code> into the pipeline.</p> <p><code>component.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\nspec:\n  ...\n  inputs:\n    - name: package\n      type: package\n  pipelineRun:\n  ...\n  params:\n    - name: oci-image-with-yaml\n      value: $(inputs.package.url)\n  workspaces:\n      - name: shared-data\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre></p> <p>The pipeline definition takes the <code>oci-image-with-yaml</code> variable from the <code>pipelineRun</code> and passes it down into the task which sets up the workspace.</p> <p>The <code>fetch-tgz-content-oci</code> task is a TAP system task which will pull an OCI image and unpack it into a workspace. As with outputs, the secrets are handled by the standard Tekton mechanism for Docker or the namespace provisioner.</p> <p><code>pipeline.yaml</code> <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\n...\nspec:\n  params:\n    - name: oci-image-with-yaml\n      type: string\n    ...\n  workspaces:\n    - name: shared-data\n  tasks:\n    - name: fetch-yaml\n      workspaces:\n        - name: store\n          workspace: shared-data\n      params:\n        - name: url\n          value: $(params.oci-image-with-yaml)\n      taskRef:\n        name: fetch-tgz-content-oci\n    - name: deployer\n      workspaces:\n        - name: content\n          workspace: shared-data\n      ...\n      taskRef:\n        name: deployer\n      runAfter:\n        - fetch-yaml\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-architecture/#pipelinerun","title":"PipelineRun","text":"<p>This section of the API partially replicates the Tekton PipelineRun API.</p> <p>The example below taken from the <code>deloyer</code> OOB component takes variables from the input, calls the <code>deployer</code> pipeline by name, sets the UIDs and GIDs for the tasks and sets the workspace to use a dynamically created persistent volume.</p> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\n...\n  pipelineRun:\n    params:\n      - name: oci-image-with-yaml\n        value: $(inputs.package.url)\n      ...\n    pipelineRef:\n      name: deployer\n    taskRunTemplate:\n      podTemplate:\n        securityContext:\n          fsGroup: 1000\n          runAsGroup: 1000\n          runAsUser: 1001\n    workspaces:\n      - name: shared-data\n        volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre> <p>When the stage is created it will create a PipelineRun from the template above.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/","title":"Tanzu Supply Chains Component Authoring","text":"<p>Engineering a component requires thinking in layers.</p> <p>This example will show how to build and test a shell script to be used in a Tekton Task. If you code reaches the level of complexity that you need to run unit test, it's recommended to use a language such as Golang or Python.</p> <ul> <li>At the bottom most layer you have the step within the Tekton task, which is a shell script. It's recommended to write the script in such a way that it can be executed and tested in isolation without the extra overhead of Tekton. </li> <li>Once the script is running then the Tekton wrapper can be place around it and it can be tested with a pipeline run.</li> <li>Once the pipeline run completes successfully it can be wrapped in a component.</li> </ul> <p>The instructions below detail the steps needed to build up these layers to create a simple stateful set based upon the pod spec output of the conventions server.</p> <p>Warning this tutorial uses ytt overlays, so you may want to check out some tutorials on that before continuing.</p> <p>This example uses simple shell scripts tested on Linux.</p> <p>The raw yaml and scripts below can be found here.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#inputs-and-outputs","title":"Inputs and outputs","text":"<p>The component will expect an input of the pod spec, which is an output from the convention server. An input from the convention server will contain a single file in the root of the workspace called <code>app-config.yaml</code>. Below is an abbreviated pod spec, with the 2 sections that will be reused.</p> <pre><code>template:\n  spec:\n    containers:\n    - env:\n      - name: JAVA_TOOL_OPTIONS\n        value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.health.probes.enabled=\"true\" -Dserver.port=\"8080\" -Dserver.shutdown.grace-period=\"24s\"\n      image: private.reg/image:tag\n...\n</code></pre> <p>To match the existing workload types and allow it to be used by the sebsequent existing supply chain components, 2 outputs will be created. <code>oci-yaml-files</code> containing the base yaml for the package and <code>oci-ytt-files</code> which will contain the variables.</p> <p>The files listed below will be expected by the <code>carvel-package</code> component.</p> <pre><code>/workspace/oci-yaml-files\n|-- appconfig.yaml # contains the base spec\n\n/workspace/oci-ytt-files\n|-- web-template-overlays.yaml # contains the overlays needed to replace values in appconfig.yaml\n|-- web-template-values.yaml # contains the base values\n</code></pre> <p>The component will also take a single variable input of the workload name.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#writing-the-script","title":"Writing the script","text":"<p>Because the script will require a file structure to be layed out, a wrapper script is needed to first setup a simulated Tekton workspace with a dummy <code>appconfig.yaml</code> to replicate a convention component output.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#simple-test-wrapper","title":"Simple test wrapper","text":"<p>The outer test wrapper script needs to setup any necessary files and export all environment variables used by the task script.</p> <p>File <code>app-config.yaml</code> is an abbreviated version of a convention component output containing the fields needed to build a K8s resource.</p> <p>Because the component has 2 outputs, 2 directories need to be passed into the script. <code>WORKSPACE_YAML</code> will be used for the <code>oci-yaml-files</code> output and <code>WORKSPACE_YTT</code> for the <code>oci-ytt-files</code> output.</p> <p>test-task-script.sh <pre><code>#!/bin/bash\n\nset -eux\n\nreadonly SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\nreadonly TEMP_DIR_YAML=\"$(mktemp -d)\"\nreadonly TEMP_DIR_YTT=\"$(mktemp -d)\"\ntrap 'rm -rf -- \"$TEMP_DIR_YAML\" &amp;&amp; rm -rf -- \"$TEMP_DIR_YTT\"' EXIT\n\ncat &lt;&lt;EOT &gt;&gt; ${TEMP_DIR_YAML}/app-config.yaml\ntemplate:\n  spec:\n    containers:\n    - env:\n      - name: JAVA_TOOL_OPTIONS\n        value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.health.probes.enabled=\"true\" -Dserver.port=\"8080\" -Dserver.shutdown.grace-period=\"24s\"\n      image: private.reg/image:tag\nEOT\n\nexport WORKSPACE_YAML=$TEMP_DIR_YAML\nexport WORKSPACE_YTT=$TEMP_DIR_YTT\nexport WORKLOAD_NAME=\"dummy\"\n\n${SCRIPT_DIR}/task-script.sh\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#task-script","title":"Task script","text":"<p>The task script expects 3 environment variables, which were set by the wrapper. <code>WORKSPACE_YAML</code> and <code>WORKSPACE_YTT</code> represent the workspace paths and <code>WORKLOAD_NAME</code> is needed to substitute into the yaml. By make these environment variables it means that the script can be Tekton agnostic and enables running/testing outside Tekton.</p> <p><code>web-template-values.yaml</code> and <code>web-template-overlays.yaml</code> are rendered using Bash to inject the workload name.</p> <p>The output <code>appconfig.yaml</code> needs to be created with ytt because it will be referencing the values from the data source from the input <code>app-config.yaml</code>.</p> <p>task-script.sh <pre><code>#!/bin/bash\n\nset -euxo pipefail\n\nTEMP_DIR=\"$(mktemp -d)\"\n\n# clean the workspace directory by moving any inputs to the temp directory\nmv $WORKSPACE_YAML/* $TEMP_DIR/\n\ncat &lt;&lt;EOT &gt;&gt; ${WORKSPACE_YTT}/web-template-values.yaml\n#@data/values-schema\n---\n#@schema/desc \"Used to generate resource names.\"\n#@schema/example \"tanzu-java-web-app\"\n#@schema/validation min_len=1\nworkload_name: \"${WORKLOAD_NAME:?}\"\n\n#@schema/desc \"Number of repicas.\"\nreplicas: 1\nEOT\n\ncat &lt;&lt;EOT &gt;&gt; ${WORKSPACE_YTT}/web-template-overlays.yaml\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n#@ load(\"@ytt:template\", \"template\")\n\n#@overlay/match by=overlay.subset({\"apiVersion\":\"apps/v1\", \"kind\": \"StatefulSet\"})\n---\nspec:\n  #@ if data.values.env:\n  #@overlay/match missing_ok=True\n  #@overlay/replace or_add=True\n  replicas: #@ data.values.replicas\n  #@ end\nEOT\n\nOUTPUT_APPCONFIG=`cat &lt;&lt;EOF\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: \"${WORKLOAD_NAME:?}\"\nspec:\n  selector:\n    matchLabels:\n      app: \"${WORKLOAD_NAME:?}\"\n  serviceName: \"${WORKLOAD_NAME:?}\"\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: \"${WORKLOAD_NAME:?}\"\n    spec:\n      containers:\n      - name: \"${WORKLOAD_NAME:?}\"\n        env: #@ data.values.template.spec.containers[0].env\n        image: #@ data.values.template.spec.containers[0].image\n        ports:\n        - containerPort: 80\n          name: web\nEOF\n`\n\necho \"$OUTPUT_APPCONFIG\" | ytt -f - --data-values-file ${TEMP_DIR}/app-config.yaml &gt; ${WORKSPACE_YAML}/appconfig.yaml\n\nls -l ${WORKSPACE_YAML}\nls -l ${WORKSPACE_YTT}\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#more-advanced-unit-testing","title":"More advanced unit testing","text":"<p>Whilst outside the scope of this article, if more advanced testing of Bash scripts are needed, it's recommended to use the BATS Framework, which will enable testing of individual functions with assertions.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#injecting-the-script-into-the-task","title":"Injecting the script into the task","text":"<p>Once the script is working exactly as expected it can be injected to the task spec.</p> <p>For this script to work the kube context be pointing to a TAP 1.9+ cluster with Tanzu Supply Chains installed.</p> <p>The script below generates an overlay to inject the variables, gets an image ref from the running cluster that container the necessary dependencies, strips the script from <code>task.yaml</code> to remove extra overlays and re-creates <code>task.yaml</code> with the injected values.</p> <p>render-task.sh <pre><code>#!/bin/bash\n\nset -eux\n\nreadonly SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\n\nreadonly TEMP_DIR=\"$(mktemp -d)\"\ntrap 'rm -rf -- \"$TEMP_DIR\"' EXIT\n\n# cp ${SCRIPT_DIR}/task.yaml-template $TEMP_DIR/task.yaml\n\nTASK_IMAGE=$(kubectl get task -n alm-catalog deployer -o jsonpath='{.spec.steps[0].image}')\n\nOVERLAY=`cat &lt;&lt;EOF &gt;&gt; ${TEMP_DIR}/overlay.yaml\n#@ load(\"@ytt:overlay\", \"overlay\")\n#@ load(\"@ytt:data\", \"data\")\n\n#@overlay/match by=overlay.subset({\"apiVersion\":\"tekton.dev/v1\", \"kind\": \"Task\"})\n---\nspec:\n  steps:\n    #@overlay/match by=overlay.index(0)\n    - image: #@ data.values.task_image\n      script: #@ data.values.script\nEOF\n`\n# Strip script to remove inline overlays\nsed -n -e '/  script:/{' -e 'p' -e ':a' -e 'N' -e '/  stepTemplate:/!ba' -e 's/.*\\n//' -e '}' \\\n  -e 'p' ${SCRIPT_DIR}/task.yaml &gt; ${TEMP_DIR}/task.yaml\n\nytt -f ${TEMP_DIR}/task.yaml -f ${TEMP_DIR}/overlay.yaml \\\n  --data-value-file script=${SCRIPT_DIR}/task-script.sh \\\n  --data-value task_image=${TASK_IMAGE} &gt; ${SCRIPT_DIR}/task.yaml\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#testing-the-task","title":"Testing the Task","text":"<p>To ensure the Tekton Task is setup correctly a dedicated Tekton pipeline can be built to run end-to-end tests.</p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#test-pipeline","title":"Test Pipeline","text":"<p>Similar to the test script the pipeline will run a task to populate the workspace, run the component task, then run a task that validate there is a line containing <code>kind: StatefulSet</code> in the correct file.</p> <p>test-pipeline.yaml <pre><code>apiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: test-pipeline\nspec:\n  params:\n  - description: Name of the Workload. Used as a default for the Carvel Package secret name.\n    name: workload-name\n    type: string\n    default: \"dummy\"\n  tasks:\n    - name: workspace-setup\n      taskSpec:\n        steps:\n          - image: blank\n            script: |\n              #!/bin/sh\n\n              cat &lt;&lt;EOT &gt;&gt; $(workspaces.shared-data.path)/app-config.yaml\n              template:\n                spec:\n                  containers:\n                  - env:\n                    - name: JAVA_TOOL_OPTIONS\n                      value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.health.probes.enabled=\"true\" -Dserver.port=\"8080\" -Dserver.shutdown.grace-period=\"24s\"\n                    image: harbor.lab:8443/tap-workload/friday-workload@sha256:cbd7c9d033f3b4a3ed5faf23a02e69bcfc9443ab405c49d9433d9af656b1eedd\n              EOT\n      workspaces:\n        - name: store\n          workspace: shared-data\n\n    - name: app-config-stateful\n      runAfter:\n        - workspace-setup\n      params:\n      - name: workload-name\n        value: $(params.workload-name)\n      taskRef:\n        name: app-config-stateful\n      workspaces:\n        - name: shared-data\n          workspace: shared-data\n        - name: overlay-data\n          workspace: overlay-data\n\n    - name: test-output\n      runAfter:\n        - app-config-stateful\n      taskSpec:\n        steps:\n          - image: blank\n            script: |\n              #!/bin/sh\n\n              grep -Fxq \"kind: StatefulSet\" $(workspaces.shared-data.path)/appconfig.yaml\n      workspaces:\n        - name: store\n          workspace: shared-data\n\n  workspaces:\n    - name: shared-data\n      description: Used to store the Conventions PodIntent and generated config files.\n    - name: overlay-data\n      description: Used to store generated YTT files.\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#setting-the-task-images","title":"Setting the task images","text":"<p>To populate the image tags run the following commands. It assumes that in <code>test-pipeline.yaml</code> the image section of each step is the first line of the step, by searching for <code>- image:</code>.</p> <pre><code>TASK_IMAGE=$(kubectl get task -n alm-catalog deployer -o jsonpath='{.spec.steps[0].image}')\nsed -i \"s%- image: .*%- image: ${TASK_IMAGE}%\" test-pipeline.yaml\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#creating-a-test-pipelinerun","title":"Creating a test PipelineRun","text":"<p>The test PipelineRun will then run through each task.</p> <p>test-pipeline-run.yaml <pre><code>apiVersion: tekton.dev/v1\nkind: PipelineRun\nmetadata:\n  name: test-pipeline-run\nspec:\n  pipelineRef:\n    name: test-pipeline\n  taskRunTemplate:\n    podTemplate:\n        securityContext:\n          fsGroup: 1000\n          runAsUser: 1001\n          runAsGroup: 1000\n  workspaces:\n    - name: shared-data\n      volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n    - name: overlay-data\n      volumeClaimTemplate:\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/component-authoring-bash/#creating-the-component","title":"Creating the component","text":""},{"location":"ci-cd/tanzu-supply-chains/hello-component/","title":"Tanzu Supply Chains Minimal Component Example","text":"<p>The example below is a hello world component that does not have any input/outputs, only a single variable to print inside the container.</p> <p>The raw yaml can be found here.</p>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#component-definition","title":"Component definition","text":"<p>The variable <code>who-dis</code> is passed from the workload into the <code>Component</code>, to the Pipeline and finally the <code>Task</code>. At execution time the WorkloadRun will create a <code>Stage</code> for each <code>Component</code>. The <code>Stage</code> will create <code>Resumptions</code> and <code>PipelineRuns</code>, with the <code>PipelineRun</code> creating <code>TaskRuns</code>.</p> <p>Whilst the <code>Task</code> could be written inline inside the pipeline, externalising it enables testing of the <code>Task</code> separately if needed.</p> <p><code>component.yaml</code> <pre><code>---\napiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: Component\nmetadata:\n  labels:\n    supply-chain.apps.tanzu.vmware.com/catalog: hello-component\n  name: hello-0.0.1\nspec:\n  config:\n  - path: spec.who-dis\n    schema:\n      type: object\n      properties:\n        name:\n          type: string\n          description: String input to be printed after \"Hello \"\n          example: bob\n      required:\n        - name\n  description: Outputs hello &lt;who-dis&gt;\n  pipelineRun:\n    params:\n    - name: who-dis\n      value: $(workload.spec.who-dis.name)\n    pipelineRef:\n      name: hello-pipeline\n    taskRunTemplate:\n      podTemplate:\n        securityContext:\n          fsGroup: 1000\n          runAsGroup: 1000\n          runAsUser: 1001\n---\napiVersion: tekton.dev/v1\nkind: Pipeline\nmetadata:\n  name: hello-pipeline\nspec:\n  params:\n  - description: Input string\n    name: who-dis\n    type: string\n  tasks:\n    - name: hello-there\n      params:\n      - name: who-dis\n        value: $(params.who-dis)\n      taskRef:\n        name: hello-task\n---\napiVersion: tekton.dev/v1\nkind: Task\nmetadata:\n  name: hello-task\nspec:\n  params:\n  - name: who-dis\n    type: string\n  steps:\n    - name: echo\n      image: alpine\n      script: |\n        #!/bin/sh\n        set -e\n        echo \"Hello $(params.who-dis)\"\n</code></pre> Apply the component.</p> <pre><code>kubectl apply -f component.yaml\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#supply-chain-spec","title":"Supply Chain Spec","text":"<p>The following yaml creates a supply chain against the component.</p> <p><code>supply-chain.yaml</code> <pre><code>apiVersion: supply-chain.apps.tanzu.vmware.com/v1alpha1\nkind: SupplyChain\nmetadata:\n  name: hello.example.tanzu-0.0.1\nspec:\n  defines: # Describes the workload\n    kind: HelloApp\n    plural: helloapps\n    group: example.tanzu\n    version: v1alpha1\n  stages: # Describes the stages\n    - name: hello\n      componentRef: # References the components\n        name: hello-0.0.1\n</code></pre></p> <p>Create and check the supply chain is ready.</p> <pre><code>kubectl apply -f supply-chain.yaml\nkubectl get supplychain hello.example.tanzu-0.0.1\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#workload-spec","title":"Workload Spec","text":"<p>The spec of a workload can be generated using the following command.</p> <pre><code>tanzu workload generate &lt;planned worklaod name&gt; -k &lt;spec.defines.plural&gt;.&lt;spec.defines.group&gt;\ntanzu workload generate hello-fred -k helloapps.example.tanzu\n</code></pre> <p>The following yaml creates a workload against the supply chain:</p> <p><code>workload.yaml</code> <pre><code>apiVersion: example.tanzu/v1alpha1\nkind: HelloApp\nmetadata:\n  name: hello-fred\nspec:\n  who-dis:\n    name: fred\n</code></pre></p>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#running-the-workload","title":"Running the workload","text":"<p>This example will deploy everything into the same namespace for simplicity. Under normal operations the supply chain and workload would be deployed into separate namespaces.</p> <p>Apply the workload.</p> <pre><code>tanzu workload create -f workload.yaml\n</code></pre> <p>Query its state.</p> <pre><code>tanzu workload get hello-fred\n</code></pre> <p>This will automatically trigger a workload run which can be monitored with the following commands. If the supply chain has inputs/outputs they can be queried here under <code>status.stages</code> using <code>kubectl</code>.</p> <pre><code>tanzu workload run get $(kubectl get helloappruns.example.tanzu \\\n  -o jsonpath='{.items[0].metadata.name}')\n\nkubectl describe helloappruns.example.tanzu\n\nkubectl get helloappruns.example.tanzu -o yaml\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#debugging","title":"Debugging","text":"<p>From TAP 1.9 the components will run in the supply chain namespace, which will be separate from the workload namespace. Under normal operation this will prevent the end user from querying Tekton resources and <code>pods</code> of the <code>stages</code> in the supply chain namespace for security reasons. If the end user needs to see workload run logs they can use the following command:</p> <pre><code>tanzu workload logs hello-fred\n</code></pre> <p>The output of the command can be limited using the following syntax:</p> <pre><code>tanzu workload logs NAME --since 1h\ntanzu workload logs NAME --since 1h --namespace default --run runname\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/hello-component/#advanced-debugging","title":"Advanced debugging","text":"<p>With access to the supply chain namespace the following commands are available.</p> <pre><code>kubectl get stage\nkubectl tree stage &lt;stage-name&gt;\nkubectl get/describe pipeline\nkubectl get/describe pipelinerun\nkubectl get/describe task\nkubectl get/describe taskrun\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/private-ca-hack/","title":"Hack Tanzu Supply Chain Tasks to Allow Private Registry","text":"<p>All tasks must be copied to the supply chain names space ahead of time for this procedure to work. This procedure will not work against any of the system namespaces, as the Carvel package will overwrite and changes made to the tasks.</p> <p>Export the supply chain namespace name:</p> <pre><code>export NS_CHAIN=\"my-supply-chain\"\n</code></pre> <p>Run the following to string substitute in the skip TLS flags. It will get all tasks inside the supply chain namespace, mutate the commands to skip TLS and apply the result back into the supply chain namespace.</p> <pre><code>kubectl get tasks -n \"${NS_CHAIN:?}\" -o yaml | \\\n  sed 's/push -i/push --registry-verify-certs=false -i/' | \\\n  sed 's/imgpkg_params -b/imgpkg_params  --registry-verify-certs=false -b/' | \\\n  sed 's/pull -i/pull --registry-verify-certs=false -i/' | \\\n  sed 's/krane config \"/krane config --insecure \"/' | \\\n  kubectl apply -n \"${NS_CHAIN:?}\" -f -\n</code></pre>"},{"location":"ci-cd/tanzu-supply-chains/supply-chain-architecture/","title":"Tanzu Supply Chains Component Architecture","text":"<p>TAP 1.8 introduced Tanzu Supply Chains as the next gen replacement for cartographer.</p> <p>In summary Tanzu Supply Chains are a wrapped for Tekton, which allow platform and DevOps engineers to easily create APIs for develops to create their workloads.</p> <p>In it's simplest form a platform engineer can create a supplychain with ~20 lines of yaml using the out of the box (OOB) supply chain components. See the tutorial to generate a supply chain using the CLI.</p> <p>If functionality is needed beyond the OOB components, then you will need to create a custom component.</p>"},{"location":"ci-cd/tanzu-supply-chains/supply-chain-architecture/#declarative-api-and-versioning","title":"Declarative API and Versioning","text":"<p>The main difference in user experience when compared to Cartographer is that platform engineers expose a fixed API for each workload type based on the components in the supply chain. As a developer I have no way to modify the supply chain beyond what is provided, like what was possible with <code>ytt</code> overlays in Cartographer.</p> <p>Each workload is versioned using standard K8s resource versioning, meaning that a platform engineer can release newer versions with API breaking changes without impacting existing workloads and give app owners the ability to move between versions.</p> <p>See the Component Architecture doc for how variables are managed.</p>"},{"location":"ci-cd/tanzu-supply-chains/supply-chain-architecture/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Components are designed to be single function and modular, meaning that as the catalogue grows, components can easily be plugged together. For example the the only component that pushes configuration to Git is the <code>git-writer-1.0.0</code> component. This is so that the logic exists once.</p>"},{"location":"ci-cd/tanzu-supply-chains/supply-chain-architecture/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>See the Component Architecture doc for a detailed description.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/","title":"Application life-cycle","text":"<p>Factor 9 of the 12 factor app states that apps must be disposable and start quickly. This means that apps must perform a graceful shutdown in a way that allows the hosting platform to make the action invisible to the client. Furthermore when running on a PaaS, an app instance must be able to signal to it's hosting platform it is in a healthy state and ready to take traffic.</p> <p>This doc will explain at a high level the actions which must be taken when engineering an app to run on Cloud Foundry, such that a user or client will not notice an interruption in service during app push, scale, restart and platform upgrades.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#what-happens-when-an-app-starts","title":"What happens when an app starts","text":"<p>The start process applies to the following events:</p> <ul> <li>cf push - new instance of a new version of the app are started to replace the old version</li> <li>cf scale - new instance of the app are started</li> <li>cf restart (rolling strategy) - all instances of the app are replaced</li> <li>Platform upgrade - Diego cells follow a rolling replacement</li> </ul> <p>At a very high level the app start process follows:</p> <ul> <li>Schedule decides a instance is needed and instructs the system to start a container</li> <li>Container starts on a Diego cell (worker)</li> <li>The Diego cell starts a health check process</li> <li>Application health check passes</li> <li>The Gorouter is instructed to add the app into rotation</li> </ul>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#application-health-checks","title":"Application health checks","text":"<p>Having a health check which only returns true when the app is ready to serve traffic is critical to ensure that adding a container does not cause a client to receive a HTTP error.</p> <p>Cloud Foundry support 3 types of health checks:</p> <ul> <li>http - a http request sent to a specific endpoint of the app, with 200 OK expected as the response</li> <li>port - a TCP can be made on a designated port or ports. This is the default.</li> <li>process - the process is running. E.g the python interpreter is running</li> </ul> <p>It is recommended to use <code>http</code> as this is the only option that can ensure the app is ready for traffic. In the case of web apps, both port and process health checks will only confirm that the web server is online, but not that the underlying software is able to respond to traffic. In addition should an app become unresponsive a port health check may return even though the underlying app is no longer able to respond to a request.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#what-happens-when-an-app-crashes","title":"What happens when an app crashes","text":"<p>In the case that an app becomes unresponsive the process is as follows:</p> <ul> <li>The Gorouter will transparently retry other instances, mark an instance as bad if it cannot make a TCP connection and take the instance out of rotation for 30 seconds</li> <li>The app health check fails</li> <li>The Gorouter is instructed to remove the app from the routing table</li> <li>The app is restarted immediately and on restart failure it follows a back-off routine.</li> </ul> <p>In the case that the Gorouter is still able to make a TCP request to an app, for example if a web service is listening, but not able to respond to the request, the Gorouter will continue to send traffic to the instance. To mitigate this it is recommended to to modify the http health check interval below the default of 30 seconds. Depending on the CPU cost of the health check there could be an impact on the platform if the value is set too low.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#what-happens-when-an-app-stops","title":"What happens when an app stops","text":"<p>The stop process applies to the following events:</p> <ul> <li>cf push - old instances of an app are stopped on a rolling basis and replaced by a new version</li> <li>cf restart (rolling strategy) - old instances of an app are stopped on a rolling basis</li> <li>cf stop - all instances of the app are stopped</li> <li>Platform scale down - Diego cells are drained and removed</li> <li>Platform upgrade - Diego cells follow a rolling replacement</li> </ul> <p>At a very high level the app shutdown process is as follows:</p> <ul> <li>The Gorouter removes the app from its routing table, meaning that no new request will be sent, but outstanding request responses will be honoured</li> <li>The scheduler instructs the Diego cell to stop the app</li> <li>The container is sent the <code>SIGTERM</code> signal, which the app should treat as a soft shutdown event and gracefully complete outstanding requests before stopping cleanly</li> <li>If after 10 seconds the container has not exited, Diego then sends a <code>SIGKILL</code> which will terminate all processes</li> </ul> <p>Should there be the need to extend the time that apps are given to shutdown this can be set system wide but will have the effect that Diego maintenance events could take longer.</p> <p>Each language will have a different way to respond <code>SIGTERM</code>.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#java-shutdown","title":"Java shutdown","text":"<p>Java allows the developer to configure pre-shutdown hooks, to insert logic into the shutdown process.</p> <p>The default behaviour in Java is as follows:</p> <ul> <li>JVM receives <code>SIGTERM</code></li> <li>All pre-shutdown hooks are triggered (if any are defined)</li> <li>The JVM will then wait for all non-daemon threads to complete before exiting</li> </ul> <p>The last point is critical, as the JVM will not exit until all theads complete, meaning the app should be designed to take this into account.</p>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#spring-annotation","title":"Spring annotation","text":"<p>Spring apps can use the @pre-destroy annoation to ensure a function is called before exiting.</p> <p>For Java 9+ the following dependency needs to be added.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;javax.annotation&lt;/groupId&gt;\n    &lt;artifactId&gt;javax.annotation-api&lt;/artifactId&gt;\n    &lt;version&gt;1.3.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#detecting-a-sigkill","title":"Detecting a SIGKILL","text":"<p>If the following line appears in app logs, then it is proof that an app was forcully shutdown by the system after the app did not respond properly to a <code>SIGTERM</code>.</p> <pre><code>OUT Exit status 137 (exceeded 10s graceful shutdown interval)\n</code></pre>"},{"location":"cloud-foundry/app-dev/app-lifecycle/#testing-app-behaviour","title":"Testing app behaviour","text":"<p>Should an app team need to test the behaviour to ensure the stop and start events are transparent to a client it is recommended to run <code>cf restart --strategy rolling</code> in a dev environment whilst the app is under load. If the app is coded, configured and scaled correctly, then the operation will be invisible to the client.</p>"},{"location":"cloud-foundry/app-dev/debugging-traffic/","title":"Debugging egress traffic","text":"<p>In the situation that there are egress traffic limitation it is necessary for a platform engineer to be able to prove where the issue is and that it is outside of the platform.</p> <p>Given that CF is able to run OCI images, a container can be build with <code>tcptraceroute</code> to enable the debugging of TCP request.</p> <p><code>tcptraceroute</code> works in similar way to <code>traceroute</code> in that it calls out all the intermediate router to get to an endpoint, but will check using a TCP port. It is useful for highlighting the router/fireall that is dropping the traffic.</p>"},{"location":"cloud-foundry/app-dev/debugging-traffic/#enabling-docker-containers","title":"Enabling Docker containers","text":"<p>By default CF doesn't allow the pushing of OCI images. It must be abled system-wide with the following command:</p> <pre><code>cf enable-feature-flag diego_docker\n</code></pre>"},{"location":"cloud-foundry/app-dev/debugging-traffic/#the-dockefile","title":"The Dockefile","text":"<p>This dockerfile can also be found here.</p> <p>It is built off the Apache2 httpd debian base image, which will render the static file containing the <code>tcptraceroute</code> output.</p> <p>The inline script runs httpd in the background and then runs an infinite loop of <code>tcptraceroute</code> commands against the defined endpoints.</p> <pre><code>FROM httpd:bookworm\n\nRUN set -xe \\\n    &amp;&amp; echo \"****** Install packages with apt ******\" \\\n    &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\\n    &amp;&amp; apt update \\\n    &amp;&amp; apt upgrade -y \\\n    &amp;&amp; apt-get install -y tcptraceroute \\\n    &amp;&amp; rm -Rf /var/lib/apt/lists/* \\\n    &amp;&amp; rm -Rf /usr/share/doc &amp;&amp; rm -Rf /usr/share/man \\\n    &amp;&amp; rm -rf /tmp/* \\\n    &amp;&amp; apt-get clean\n\nRUN cat &lt;&lt;EOF &gt;&gt; /root/check.sh\n#!/bin/bash\nset -eu\nif [ -z \\${ENDPOINT+x} ] || [ -z \\${ENDPOINT_TCP_PORT+x} ]; then \n  echo \"You must set ENDPOINT and ENDPOINT_TCP_PORT environment variabels\"\n  exit 1\nfi\n\nrm -f /usr/local/apache2/htdocs/index.html\nhttpd-foreground &amp;\n\nwhile true; do \n  tcptraceroute \\${ENDPOINT} \\${ENDPOINT_TCP_PORT} 2&gt;&amp;1 | tee -a /usr/local/apache2/htdocs/index.html\n  echo \"&lt;/br&gt;&lt;/br&gt;\" &gt;&gt; /usr/local/apache2/htdocs/index.html\n  sleep 5\ndone\nEOF\n\nRUN chmod +x /root/check.sh\n\nENTRYPOINT [\"/root/check.sh\"]\n</code></pre> <p>The image should be built as normal and pushed to a container registry.</p> <pre><code>docker image build . -t my-repo/tcp-tst:latest\ndocker push my-repo/tcp-tst:latest\n</code></pre>"},{"location":"cloud-foundry/app-dev/debugging-traffic/#pushing-the-app","title":"Pushing the app","text":"<p>This </p> <pre><code>export CF_DOCKER_PASSWORD=&lt;my-registry-password&gt;\ncf push tcp-test --docker-image my-repo/tcp-tst:latest \\\n  --docker-username &lt;my-registry-username&gt; \\\n  --no-start\n\ncf set-env tcp-test ENDPOINT google.com\ncf set-env tcp-test ENDPOINT_TCP_PORT 443\n\ncf start tcp-test\n</code></pre>"},{"location":"cloud-foundry/app-dev/debugging-traffic/#debugging","title":"Debugging","text":"<p>The app logs will contain the outputs, plus the app can accessed via a web browsers. </p> <p>Note that the browsers won't add all character returns, meaning the app logs will give a cleared output.</p>"},{"location":"cloud-foundry/bosh/relocating-vms/","title":"Relocating VMs in vSphere","text":"<p>The procedure will describe how to relocate standard VMs from one cluster to another, between vCenters if necessary.</p> <p>The author cannot take responsibiliy for any issues that could result from making this change. It is recommended to test in a lab environment and check with Broadcom support before rolling out in production.</p> <p>This process has been tested using the Harbor tile and has not been tested against service instance tiles, such as data services or TKGI.</p>"},{"location":"cloud-foundry/bosh/relocating-vms/#requirements","title":"Requirements","text":"<ul> <li>This process was tested using Opsman 3.0.33.</li> <li>It is strongly recommended to use BBR to backup the director and any tiles which will be changed</li> <li>There must be a shared datastore that exists on both the source and target cluster<ul> <li>Both source and target datastore MUST have the SAME name</li> </ul> </li> <li>The network must be available on the source and target clusters<ul> <li>Both source and target port group MUST have the SAME name.</li> </ul> </li> <li>Tested with NSX being disabled in the vCenter config. If NSX is enabled, then extra testing would be required!</li> </ul>"},{"location":"cloud-foundry/bosh/relocating-vms/#procedure","title":"Procedure","text":""},{"location":"cloud-foundry/bosh/relocating-vms/#move-the-disks-to-the-shared-datastore","title":"Move the disks to the shared datastore","text":"<p>This step will upload the stemcell to the new datastore, re-create the VMs with ephemeral snapshots from the new datastore, attach new disks from the new datastores to copy persistent data and finally detach the source persistent disk.</p> <ul> <li>Update the persistent disk and ephemeral disk on the Bosh director \"vCenter Config\" tab to the migration datastore</li> <li>On the \"Director Config\" tab check <code>Recreate VMs deployed by the BOSH Director</code></li> <li>Apply changes to move the Bosh director and all non-service instance VMs to the new disk</li> </ul>"},{"location":"cloud-foundry/bosh/relocating-vms/#recreate-all-the-vms-on-the-target-cluster","title":"Recreate all the VMs on the target cluster","text":"<p>This step will force all VMs to be re-created on the target vSphere cluster.</p> <ul> <li>Add the target cluster as an availability zone (if necessary add the target vCenter, taking care to specify the correct datastores)</li> <li>Edit the required networks on the Bosh \"Create Networks\" page, to add the new availability zone</li> <li>On the Opsman VM edit <code>/var/tempest/workspaces/default/deployments/bosh-state.json</code><ul> <li>Take a backup of the file</li> <li>Remove the <code>stemcells</code> section to force a re-upload on the next apply changes</li> </ul> </li> <li>On the \"Director Config\" tab check <code>Recreate VMs deployed by the BOSH Director</code> (this gets cleared after the previous successful apply changes).</li> <li>(If a static IP is define on the Harbor deployment) Remove it by setting it to blank.</li> <li>Enable Opsman Advanced mode</li> <li>Update the assigned availibility zones of the Bosh director and any tiles that need to be moved</li> <li>Cleanly shut down the Bosh director (not any other VMs)</li> <li>Apply changes</li> <li>Disable Opsman Advanced mode or wait for it to timeout</li> <li>(If a static IP is define on the Harbor deployment) add it back again and apply changes.</li> </ul>"},{"location":"cloud-foundry/bosh/relocating-vms/#tidy-up","title":"Tidy Up","text":"<p>This step will remove the orginal Bosh director VM and move data to the target datastore.</p> <ul> <li>On the source cluster edit the Bosh director VM to detach (not delete) the presistent disk</li> <li>Delete the source Bosh director VM</li> <li>Update the persistent disk and ephemeral disk on the Bosh director \"vCenter Config\" tab to the target datastore</li> <li>On the \"Director Config\" tab check <code>Recreate VMs deployed by the BOSH Director</code></li> <li>Apply changes to move the Bosh director and all non-service instance VMs to the target disk</li> </ul>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/","title":"Run pre-start scripts with Bosh <code>runtime-config</code> and the <code>os-conf</code> release","text":"<p>The procedure explains how to run pre-start commands on a Bosh instance group. For example to update a config file before a service starts.</p> <p>In this example it is used to set a flag in the Healthwatch v2 Grafana <code>defaults.ini</code> file before Grafana starts. Bosh runtime-config will run a pre-start script only on the grafana instance to modify the file before the service is allowed to start.</p> <p>The reason for using Bosh <code>runtime-config</code> is so that Opsman cannot overwrite it during the next apply changes. Bosh <code>runtime-config</code> works as a template to apply over the top of any Opsman or Bosh deployment, meaning it is completely independent. It needs to be applied once and will continue to work for the life of the Bosh director.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#dependencies","title":"Dependencies","text":"<ul> <li>This process assumes an install of Healthwatch v2 installed by Bosh via Opsman.</li> <li>bosh and om CLIs.</li> <li>Connection to the Opsman VM over https and ssh, plus admin credentials and the ssh key.</li> </ul>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#process","title":"Process","text":"<ol> <li>Connect to the Bosh director</li> <li>Upload the the <code>os-conf</code> Bosh release to the director</li> <li>Create a <code>runtime-config</code> with a pre-start script</li> <li>Trigger a deployment to have the <code>runtime-config</code> injected into the deployment</li> </ol> <p>Note that any <code>os-conf</code> job can only be run once per instance. This means that only one pre-start script can be run. This is something to consider if more than one modification is needed, as they will need to be grouped together in the same <code>runtime-config</code>.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#steps","title":"Steps","text":"<p>This assumes and existing installation of Healthwatch.</p> <p>Steps 1-3 could be followed before Healthwatch is installed, allowing it to be added to an installation workflow without triggering a second deployment.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#step-1-connect-to-the-bosh-director","title":"Step 1 - Connect to the Bosh Director","text":"<p>Export the Opsman credentials:</p> <pre><code>OM_USERNAME=admin\nOM_TARGET=&lt;opsman-ip/fqdn&gt;\nOM_SKIP_SSL_VALIDATION=True # if using self signed\nOM_PASSWORD=&lt;password&gt;\n</code></pre> <p>Choose the connection type depending whether the local machine has network access to the Bosh director.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#connect-to-the-bosh-director-directly-from-local-machine","title":"Connect to the Bosh Director directly from local machine","text":"<p>Export the Bosh variables in the shell session buy running:</p> <pre><code>eval \"$(om bosh-env)\"\n</code></pre>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#connect-to-the-bosh-director-via-opsman-proxy","title":"Connect to the Bosh Director via Opsman proxy","text":"<p>Export the Bosh variables in the shell session, including the Opsman SSH key for proxying buy running, substituting in the path to the Opsman SSH private key:</p> <pre><code>eval \"$(om bosh-env --ssh-private-key=&lt;path-to-opsman-ssh-key&gt;)\"\n</code></pre> <p>This will make the Bosh CLI proxy commands over SSH through the Opsman VM to the Bosh director.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#step-3-upload-the-os-conf-bosh-release","title":"Step 3 - Upload the <code>os-conf</code> Bosh release","text":"<p>This step will upload the <code>os-conf</code> release to the Bosh director.</p>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#when-internet-connected","title":"When internet connected","text":"<pre><code>bosh upload-release --sha1 daf34e35f1ac678ba05db3496c4226064b99b3e4 https://bosh.io/d/github.com/cloudfoundry/os-conf-release?v=22.2.1\n</code></pre>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#when-ait-gapped","title":"When ait gapped","text":"<p>Download the release file from here.</p> <p>Transfer the file downloaded file to a machine with Bosh connectivity. <code>os-conf-release-22.2.1.tgz</code> was the latest at time of writing.</p> <pre><code>bosh upload-release os-conf-release-22.2.1.tgz\n</code></pre>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#step-3-set-the-runtime-config","title":"Step 3 - Set the <code>runtime-config</code>","text":"<p>See the appendix for an explanation of the yaml.</p> <p>Create the following file:</p> <p><code>grafana-runtime-config.yaml</code> <pre><code>releases:\n- name: os-conf\n  version: latest\naddons:\n  - name: grafana-update\n    include: \n      instance_groups:\n        - grafana\n    jobs:\n      - name: pre-start-script\n        release: os-conf\n        properties:\n          script: |-\n              #!/bin/bash\n              sed -i -e 's/viewers_can_edit = false/viewers_can_edit = true/' /var/vcap/packages/grafana/conf/defaults.ini\n</code></pre></p> <p>Apply the config to Bosh. Note the update command will create/update, so can always be used.</p> <pre><code>bosh update-config --type runtime --name grafana-conf grafana-runtime-config.yaml\n</code></pre>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#step-4-trigger-a-bosh-deployment","title":"Step 4 - Trigger a Bosh deployment","text":"<p>To inject the <code>runtime-config</code> a Bosh deployment must be triggered. Simply re-creating the Grafana instance will not be enough, as it will re-deploy using the existing manifest.</p> <p>Trigger an apply changes and watch the logs to see the injection of the <code>runtime-config</code>.</p> <pre><code>om apply-changes --product-name p-healthwatch2\n</code></pre>"},{"location":"cloud-foundry/bosh/run-pre-start-commands/#appendix","title":"Appendix","text":""},{"location":"cloud-foundry/bosh/run-pre-start-commands/#yaml-explanation","title":"Yaml explanation","text":"<p>This section will explain the composition of <code>grafana-runtime-config.yaml</code>.</p> <p>The <code>release</code> section tells Bosh to load the latest version of the <code>os-conf</code> release.</p> <pre><code>releases:\n- name: os-conf\n  version: latest\n</code></pre> <p><code>addon</code> defines a list of jobs to add to instances. The name of the add-on is set to <code>grafana-update</code>.</p> <p><code>include</code> specifies the <code>grafana</code> instance_group, so that this add-on will only apply to the grafana instance groups. Other placement rules are available.</p> <pre><code>addons:\n  - name: grafana-update\n    include: \n      instance_groups:\n        - grafana\n</code></pre> <p><code>jobs</code> includes a single job. </p> <p>The <code>name</code> of the job has to match on of the available os-conf jobs. In this case <code>pre-start-script</code> is used.</p> <p>The <code>pre-start-script</code> will run after releases have been loaded, but before monit starts any services. See job lifecycle for more details.</p> <p><code>releases</code> specifies to use the <code>os-conf</code> release.</p> <p><code>properties.script</code> contains a simple shell script which will string substitute <code>viewers_can_edit = true</code> into <code>/var/vcap/packages/grafana/conf/defaults.ini</code>.</p> <pre><code>    jobs:\n      - name: pre-start-script\n        release: os-conf\n        properties:\n          script: |-\n              #!/bin/bash\n              sed -i -e 's/viewers_can_edit = false/viewers_can_edit = true/' /var/vcap/packages/grafana/conf/defaults.ini\n</code></pre>"},{"location":"cloud-foundry/gemfire/gemfire-dashboard/","title":"Monitoring GemFire with Healthwatch 2","text":"<p>To monitor the state of Gemfire replication it is recommended to monitor the Sender Event Size and trigger an alarm if it goes signifficantly over the historical peak.</p> <p>To achieve this:</p> <ul> <li>Add a dashboard that allows monitoring historical event queue size</li> <li>Setup an alert trigger it it goes signifficantly above the historical queue size</li> </ul>"},{"location":"cloud-foundry/gemfire/gemfire-dashboard/#viewing-the-event-queue","title":"Viewing the event queue","text":"<p>Add this dashboard to the Healthwatch Grafana instance.</p> <p>Look at the \"Event Queue Size\" for the past 7 days and note down how high it goes under stead state. Using discression add an amount on top to define the alerting threshold, e.g. +100%.</p>"},{"location":"cloud-foundry/gemfire/gemfire-dashboard/#setting-alert-triggers","title":"Setting alert triggers","text":"<p>From the hamburger pull out menu expand \"Alerting\" and click \"Alerting rules\". Click the \"New alert rule\" button.</p> <p>In the dialog fill in section 2:</p> <ul> <li>Make sure the query it set to \"Code\" mode</li> <li>Paste <code>{__name__=~\"gatewaySender.*_EventQueueSize\"}</code> into the query field</li> <li>Delete the \"Reduce\" expression if present</li> <li>Update the \"Threshold\" expression to have <code>A</code> as the \"Input\"</li> <li>The the \"Is above\" value in teh \"Threshold\" expression to the value derived from the historical queue size</li> </ul> <p>E.g.:</p> <p></p> <p>Click the \"Preview\" button to confirm that the query matches metrics and that all the metrics show as normal.</p> <p>E.g.:</p> <p></p> <p>Continue to fill in the remaining alert settings.</p>"},{"location":"cloud-foundry/gemfire/gemfire-dashboard/#optional-add-a-lower-theshold-to-know-whether-to-adjust-the-red-alert-threshold","title":"(optional) Add a lower theshold to know whether to adjust the red alert threshold","text":"<p>Repeat the process above using a lower threshold to trigger a yellow alert, to use as an indicator that the red alert threshold needs to be adjusted.</p>"},{"location":"cloud-foundry/logging-monitoring/disableing-firehose-logs/","title":"Disabling Logging on the Firehose in TP-CF 2.13+","text":""},{"location":"cloud-foundry/logging-monitoring/disableing-firehose-logs/#background","title":"Background","text":"<p>From TP-CF 2.13 the logging architect changed to allow a shared nothing topology.</p> <p></p> <p>This change enables all app logs being emitted as syslogs to completely bypass the Loggregator Firehose architecture. Whilst this simplification has the potential to reduce the system footprint, there are some considerations before the \"Do not forward app logs to the Firehose\" can be ticked on the TAS tile.</p>"},{"location":"cloud-foundry/logging-monitoring/disableing-firehose-logs/#considerations","title":"Considerations","text":"<p>All app logs being sent externally must be sent externally must be sent via syslog. None of the partner Nozzle tiles must be in use. E.g. (Azure Nozzle, Nee Relic Nozzle, Splunk Nozzle, Sumo Nozzle).</p> <p>App Metrics currently uses the Firehose for logs which is also a blocker.</p> <p>Support for Log Cache was added to CF CLI v6.50.0, meaning anyone using the prior versions of the CF CLI will need to upgrade to be able to continue to stream logs via the CF CLI.</p>"},{"location":"cloud-foundry/uaa/login/","title":"UAA Login to Tanzu products","text":"<p>This doc explains how to use the Golang based uaa-cli to log into Tanzu products.</p>"},{"location":"cloud-foundry/uaa/login/#dependencies","title":"Dependencies","text":"<ul> <li>uaa-cli</li> <li>om CLI</li> <li>jq</li> <li>Connection to the Opsman VM over https/ssh, uaa over port 84434, Opsman admin credentials and the ssh key.</li> </ul>"},{"location":"cloud-foundry/uaa/login/#set-opsman-environment-vars","title":"Set Opsman environment vars","text":"<pre><code>OM_USERNAME=admin\nOM_TARGET=&lt;opsman-ip/fqdn&gt;\nOM_SKIP_SSL_VALIDATION=True # if using self signed\nOM_PASSWORD=&lt;password&gt;\n</code></pre>"},{"location":"cloud-foundry/uaa/login/#logging-in-to-bosh-uaa","title":"Logging in to Bosh UAA","text":"<p><code>om bosh-env</code> can be used to populate the <code>BOSH_</code> environment variables, as it will also get the Bosh director IP address.</p> <pre><code>eval \"$(om bosh-env)\"\nuaa target \"https://${BOSH_ENVIRONMENT}:8443\" --verbose -k\nuaa get-client-credentials-token ops_manager -s \"${BOSH_CLIENT_SECRET}\"\n</code></pre>"},{"location":"cloud-foundry/uaa/login/#logging-in-to-tas-uaa","title":"Logging in to TAS UAA","text":"<p>The IP address of the TAS UAA servers must be exported as <code>TAS_UAA_ADDRESS</code>. </p> <p>To find the IP address go to the TAS tile in the Opsman UI, Status, then find one of </p> <p>E.g.</p> <pre><code>export TAS_SYS_DOMAIN=\"sys.192.168.1.229.nip.io\"\n</code></pre> <pre><code>UAAC_SECRET=\"$(om -k credentials --product-name cf \\\n  --credential-reference .uaa.admin_client_credentials --format=json |jq -r '.password')\"\nuaa target https://uaa.${TAS_UAA_ADDRESS} --skip-ssl-validation\nuaa get-client-credentials-token admin -s $UAAC_SECRET\n</code></pre>"},{"location":"cloud-foundry/uaa/login/#logging-in-to-tkgi-uaa","title":"Logging in to TKGI UAA","text":"<p>The IP address of the TKGI API servers must be exported as <code>TKGI_ADDRESS</code>.</p> <p>To find the IP address go to the TKGI tile in the Opsman UI, Status, then the TKGI API server.</p> <p>E.g.</p> <pre><code>export TKGI_ADDRESS=\"192.168.0.10\"\n</code></pre> <pre><code>UAAC_SECRET=$(om -k credentials --product-name pivotal-container-service \\\n  --credential-reference .properties.pks_uaa_management_admin_client --format=json | yq r - secret | tr -d '\"')\nuaa target https://${TKGI_ADDRESS}:8443 --skip-ssl-validation\nuaa get-client-credentials-token admin -s $UAAC_SECRET\n</code></pre>"},{"location":"k8s/apps/minimal-lb/","title":"Minimal Kubernetes web app + LB","text":"<p>The following yaml will deploy a basic web server and expose it via a load balancer service.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  labels:\n    app: test-pod\nspec:\n  containers:\n  - name: test-webserver\n    image: k8s.gcr.io/test-webserver:latest\n    ports:\n    - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-svc\nspec:\n  selector:\n    app: test-pod\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"k8s/tkgs/admin-access/","title":"How to access the Tanzu with vSphere Products","text":""},{"location":"k8s/tkgs/admin-access/#ssh-to-the-supervisor-cluster-nodes","title":"SSH to the Supervisor Cluster nodes","text":"<p>SSH to the vCenter as root and run to get supervisor creds <code>/usr/lib/vmware-wcp/decryptK8Pwd.py</code>.</p> <p>So long as firewall rules allow, the credentials can be saved and the connection made to the supervisor without access to the vCenter.</p>"},{"location":"k8s/tkgs/admin-access/#ssh-to-the-guest-cluster-nodes","title":"SSH to the Guest Cluster nodes","text":"<p>Commands should be run on the supervisor cluster.</p> <p>SSH commands can be run from anywhere with access to the VMs.</p>"},{"location":"k8s/tkgs/admin-access/#using-private-keys","title":"Using private keys","text":"<p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh -o jsonpath={.data.\"ssh-privatekey\"}  | base64 -d | tee -a privatekey.key\nchmod 600 /tmp/privatekey.key\nssh -i /tmp/privatekey.key vmware-system-user@&lt;guest_IP&gt;\n</code></pre> The private key can be exported and if not should be deleted when finished.</p>"},{"location":"k8s/tkgs/admin-access/#using-the-password","title":"Using the password","text":"<pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh-password -o jsonpath={.data.\"ssh-passwordkey\"} | base64 -d\nssh vmware-system-user@&lt;guest_IP&gt;\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#running-commands-without-kubectl-for-debugging","title":"Running commands without kubectl, for debugging","text":"<p><code>crictl</code> mostly mirrors <code>docker</code></p> <pre><code>sudo -i\ncrictl ps # show running containers\ncrictl logs &lt;container ID&gt; # show container logs\ncrtictl exec pod &lt;container ID&gt; /bin/sh # exec into a container\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#getting-kubectl-inside-a-workload-cluster-control-plane-node","title":"Getting kubectl inside a workload cluster control plane node","text":"<pre><code>sudo -i\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"k8s/tkgs/admin-access/#legacy-tkgs-harbor-admin-ui-access","title":"Legacy TKGS Harbor Admin UI Access","text":"<p><code>kubectl</code> commands must be run from inside a supervisor cluster VM.</p> <pre><code>HARBOR_NAMESPACE=$(kubectl get ns | grep registry- | awk '{print $1}')\nHARBOR_POD_ID=$(echo $HARBOR_NAMESPACE | sed 's/.*-//')\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminUsername}' |base64 -d |base64 -d\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminPassword}' |base64 -d |base64 -d\n</code></pre>"},{"location":"k8s/tkgs/repave-on-7/","title":"Re-pave a vSphere 7 cluster","text":"<p>This patch will cause all nodes to be created on a vSphere 7 workload cluster.</p> <p>Commands must be run against the supervisor.</p>"},{"location":"k8s/tkgs/repave-on-7/#find-the-machinedeployments-object-for-your-cluster","title":"Find the <code>machinedeployments</code> object for your cluster","text":"<pre><code>kubectl get machinedeployments.cluster.x-k8s.io -n &lt;namespace&gt;\n</code></pre> <pre><code>kubectl patch machinedeployments.cluster.x-k8s.io ${TEMPLATE_NAME:?} --type merge -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\"`date +'%s'`\\\"}}}}}\" -n &lt;namespace&gt;\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/","title":"Set default PSA on vSphere with Tanzu 1.26+ clusters","text":"<p>From TKR 1.26 and above vSphere with Tanzu changed the default Pod Security Admission (PSA) level to enforced, meaning that each namespace must be labelled to relax the policy. The instructions on this page explain how to create a custom cluster class from the default class which sets the default PSA level to audit.</p>"},{"location":"k8s/tkgs/set-default-psa/#warning","title":"Warning","text":"<p>Taken from the vSphere with Tanzu docs.</p> <p>\"Custom ClusterClass is an experimental Kubernetes feature per the upstream Cluster API documentation. Due to the range of customizations available with custom ClusterClass, VMware cannot test or validate all possible customizations. Customers are responsible for testing, validating, and troubleshooting their custom ClusterClass clusters. Customers can open support tickets regarding their custom ClusterClass clusters, however, VMware support is limited to a best effort basis only and cannot guarantee resolution to every issue opened for custom ClusterClass clusters. Customers should be aware of these risks before deploying custom ClusterClass clusters in production environments.\"</p> <p>Given the statement above and the fact that a cluster cannot currently be switched between cluster classes, it is not recommended to use custom cluster classes.</p> <p>The procedure is based on the {vSphere docs](https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-with-tanzu-tkg/GUID-EFE7DB40-8748-42B5-9694-DBC21F9FB76A.html), which you should always reference to check for changes.</p>"},{"location":"k8s/tkgs/set-default-psa/#procedure","title":"Procedure","text":""},{"location":"k8s/tkgs/set-default-psa/#step-1-copy-the-default-clusterclass","title":"Step 1 - Copy the default ClusterClass","text":"<p>Export the variables to match your environment.</p> <pre><code>export NS=\"ns1\"\nexport CCC_NAME=\"my-cc\"\n</code></pre> <p>Export the default ClusterClass, strip unnecessary fields and update the name. <pre><code>kubectl -n $NS get clusterclass tanzukubernetescluster -o yaml &gt; ccc.yaml\nsed -i '/creationTimestamp:/d' ccc.yaml &amp;&amp; sed -i '/generation:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml &amp;&amp; sed -i '/uid:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml\nsed -i \"s/  name: tanzukubernetescluster/  name: ${CCC_NAME}/g\" ccc.yaml\n</code></pre></p>"},{"location":"k8s/tkgs/set-default-psa/#step-2-modify-the-custom-clusterclass","title":"Step 2 - Modify the custom ClusterClass","text":"<p>It's recommended to manually edit the file to set policy, but automated step are listed below.</p> <ul> <li>Open ccc.yaml in your favourity editor.</li> <li>Search for <code>controlPlaneFilesAdmissionConfigurationk8s126</code> and scroll up to see the <code>AdmissionConfiguration</code> template.</li> <li>Modify the yaml to set your policy by updating the section <code>plugins.0.configuration.defaults</code>. Scrolling up 30 lines will show the K8s 1.25 policy which does not enforce.</li> </ul>"},{"location":"k8s/tkgs/set-default-psa/#automated-steps-to-replace-out-the-fields-of-the-k8s-125-policy","title":"Automated steps to replace out the fields of the K8s 1.25 policy.","text":"<p>Use with caution as this was only tested with 1.26.</p> <pre><code>sed -i -E 's/enforce: \"restricted\"/warn: \"restricted\"\\n                      warn-version: \"latest\"/' ccc.yaml\nsed -i -E 's/enforce-version: \"latest\"/audit: \"restricted\"\\n                      audit-version: \"latest\"/' ccc.yaml\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/#step-3-apply-the-custom-clusterclass-to-any-namespace","title":"Step 3 - Apply the custom ClusterClass to any namespace","text":"<p>The ClusterClass to any namespaces where it is needed.</p> <pre><code>export TARGET_NS=\"ns2\"\nsed -i \"s/namespace: .*/namespace: ${TARGET_NS}/g\" ccc.yaml\nkubectl apply -f ccc.yaml\n</code></pre>"},{"location":"k8s/tkgs/set-default-psa/#step-4-create-clusters-using-the-custom-clusterclass","title":"Step 4 - Create clusters using the custom ClusterClass","text":"<p>Add the following section to your ClusterClass yamls <pre><code>spec:\n  topology:\n    class: &lt;custom cluster class name&gt;\n</code></pre></p>"},{"location":"k8s/tkgs/troubleshooting/","title":"Bootstrap troubleshooting","text":"<p>SSH onto s supervisor node and <code>sudo -i</code>.</p> <p>Check containers with:</p> <pre><code>crictl ps\n</code></pre> <p>Check if the api server is available.</p> <pre><code>kubectl get pod -A\n</code></pre> <p>Check the state of services and the state of cloud init:</p> <pre><code>systemctl --type service\n\nless /var/log/cloud-init-output.log\njournalctl -xeu  cloud-final\njournalctl -xeu  cloud-init\njournalctl -xeu  cloud-config\njournalctl -xeu  cloud-init-local\n</code></pre> <p>Check logs on all the nodes with:</p> <pre><code>grep -R -i stderr /var/log/pods/*\ngrep -R -i error /var/log/pods/*\ngrep -R -i fail /var/log/pods/*\n</code></pre> <p>Check Kubelet</p> <pre><code>systemctl status kubelet  --no-pager --full\njournalctl -xeun kubelet\ncrictl images ls\n</code></pre>"},{"location":"k8s/tkgs/update-cp-disks/","title":"How to update the control plane disks of a workload cluster on vSphere with Tanzu","text":"<p>The docs explains how to expand the disk of a vSphere with Tanzu control plane VMs in place.</p> <p>THIS PROCEDURE IS NOT SUPPORTED BY VMWARE BY BROADCOM AND MUST BE USED WITH CAUTION! THE AUTHOR CANNOT BE HELD RESPONSIBLE FOR ANY ISSUES.</p> <p>Running without the update validation webhook is risky as no control plane update actions will be validated, so should be done for the minimum amount of time..</p>"},{"location":"k8s/tkgs/update-cp-disks/#connect-to-a-supervisor-vm","title":"Connect to a Supervisor VM","text":"<p>SSH into the vCenter as root and enter a shell.</p> <p>Run: <pre><code>/usr/lib/vmware-wcp/decryptK8Pwd.py\n</code></pre></p> <p>SSH into the IP address provided by the script output as the root user, using the password provided.</p>"},{"location":"k8s/tkgs/update-cp-disks/#on-a-supervisor-vm","title":"On a supervisor VM","text":"<p>Run: <pre><code>kubectl get ValidatingWebhookConfiguration capi-kubeadm-control-plane-validating-webhook-configuration -o yaml &gt; cp-hook-org.yml\ncp cp-hook-org.yml cp-hook-upd.yml\nkubectl get ValidatingWebhookConfiguration vmware-system-tkg-validating-webhook-configuration -o yaml &gt; tkg-hook-org.yml\ncp tkg-hook-org.yml tkg-hook-upd.yml\n</code></pre></p> <p>Edit <code>cp-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: validation.kubeadmcontrolplane.controlplane.cluster.x-k8s.io</code> (this was the first webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Edit <code>tkg-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: default.validating.tanzukubernetescluster.run.tanzu.vmware.com</code> (this was the last webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Run: <pre><code>kubectl apply -f cp-hook-upd.yml\nkubectl apply -f tkg-hook-upd.yml\n</code></pre></p>"},{"location":"k8s/tkgs/update-cp-disks/#updating-workload-cluster-via-supervisor-manifest","title":"Updating workload cluster via Supervisor manifest","text":"<p>On a standard linux machine with a normal connection to the supervisor</p> <p>Now it is possible to update the control plane disks. Be careful to not make any other changes.</p>"},{"location":"k8s/tkgs/update-cp-disks/#updating-class-based-workload-clusers-via-tmc","title":"Updating Class based workload clusers via TMC","text":"<p>Instructions use the Tanzu CLI.</p> <p>Export the existing cluster config to yaml: <pre><code>tanzu management-cluster cluster get &lt;cluster_name&gt; -p &lt;supervisor_namesapce&gt; -m &lt;tmc_supervisor_name&gt; -o yaml &gt; cluster_spec.yaml\n</code></pre></p> <p>Remove the \"meta\" and \"status\" sections.</p> <p>Under spec.topology.variables add the following (update the mount, storageClass and sizes as necessary): <pre><code>spec:\n  topology:\n    variables:\n    - name: controlPlaneVolumes\n      value:\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/containerd\n        name: containerd\n        storageClass: tkgs-storage-policy\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/kubelet\n        name: kubelet\n        storageClass: tkgs-storage-policy\n</code></pre></p>"},{"location":"k8s/tkgs/update-cp-disks/#revert-on-completion","title":"Revert on completion!!!!","text":"<p>Once finished, revert the changes on the supervisor by addding the <code>- UPDATE</code> lines that were removed at the start.</p> <p>You can re-apply the original files by SSHing back to the same supervisor node, which should happen automatically following the connection procedure. </p> <p>Run: <pre><code>kubectl apply -f cp-hook-org.yml\nkubectl apply -f tkg-hook-org.yml\n</code></pre></p> <p>If for any reason you are not able to access the original yaml files you can follow the process in reverse by adding the relevant lines.</p>"},{"location":"shell/bash-recipes/","title":"Bash Recipes","text":""},{"location":"shell/bash-recipes/#run-command-with-stdin-as-an-input","title":"Run command with stdin as an input","text":"<p>Kubbectl apply from an inline yaml string.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: tap-install\nEOF\n</code></pre>"},{"location":"shell/bash-recipes/#conditional-run-command","title":"Conditional run command","text":"<p>Run command based on output of another command. </p> <pre><code>set +e\nnetworks=\"$(docker network inspect kind-tilt 2&gt;&amp;1)\"\nset -e\nif [[ \"$networks\" == *\"network kind-tilt not found\"* ]]; then\n  docker network create --gateway=\"100.127.0.1\" --ip-range=\"100.127.0.0/24\" --subnet=\"100.127.0.0/16\" kind-tilt\nelse\n  echo \"kind-tilt network already exists\"\nfi\n</code></pre>"},{"location":"shell/bash-recipes/#sed","title":"Sed","text":""},{"location":"shell/bash-recipes/#get-all-on-line-text-after-a-string","title":"Get all on line text after a string","text":"<p>E.g. return value after the first occurrence of <code>name:</code> from a file on Linux.</p> <pre><code>sed -n -e '/name/ {s/.*: *//p;q}' /$HOME/.kube/config\n</code></pre>"},{"location":"shell/bash-recipes/#replace-text-in-a-line-keeping-before-the-match","title":"Replace text in a line, keeping before the match","text":"<p>E.g. </p> <pre><code>sed -r 's/(^.*)cheese/\\1mice/'\n</code></pre>"},{"location":"shell/bash-recipes/#remove-all-lines-starting-with-string","title":"Remove all lines starting with string","text":"<p>E.g. remove all lines starting with \"#@\".</p> <pre><code>cat blah.txt | grep -v \"#~\"\n</code></pre>"},{"location":"shell/bash-recipes/#check-if-a-url-is-available","title":"Check if a URL is available","text":"<p>E.g. check if google.com is contactable. Can be used to check if VPN is connected.</p> <pre><code>status_code=\"$(curl --write-out %{http_code} --silent --output -k /dev/null https://www.google.com)\"\n\nif [[ \"$status_code\" -ne 200 ]] ; then\n  echo \"You are not connected to the VPN. Please connect and re-run this script\"\nelse\n  exit 1\nfi\n</code></pre>"},{"location":"shell/bash-recipes/#check-if-command-is-in-path","title":"Check if command is in path","text":"<p>E.g. check if ytt is available in the shell.</p> <pre><code>if ! [ -x \"$(command -v ytt)\" ]; then\n  echo -e 'ytt CLI not in path.\\nSee https://github.com/carvel-dev/ytt' &gt;&amp;2\n  exit 1\nfi\n</code></pre>"},{"location":"shell/bash-recipes/#do-something-if-command-fails","title":"Do something if command fails","text":"<p>E.g. ff image is not in local registry copy image over.</p> <pre><code>if ! docker image inspect ${INSTALL_REGISTRY_HOSTNAME}/${INSTALL_REPO}/tap-packages:${TAP_VERSION} &gt; /dev/null; then\n  imgpkg copy -b registry.tanzu.vmware.com/tanzu-application-platform/tap-packages:${TAP_VERSION:-?} --to-repo ${INSTALL_REGISTRY_HOSTNAME:?}/${INSTALL_REPO:?}/tap-packages\nfi\n</code></pre>"},{"location":"tanzu-platform/intro/","title":"Tanzu Platform","text":"<p>Tanzu Platform is the collective solution covering all Tanzu products.</p>"}]}