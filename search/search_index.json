{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>This page is a collection of useful patterns and tutorials I've collected over the years.</p>"},{"location":"tkgs/admin-access/","title":"How to access the Tanzu with vSphere Products","text":""},{"location":"tkgs/admin-access/#ssh-to-the-supervisor-cluster-nodes","title":"SSH to the Supervisor Cluster nodes","text":"<p>SSH to the vCenter as root and run to get supervisor creds <code>/usr/lib/vmware-wcp/decryptK8Pwd.py</code></p>"},{"location":"tkgs/admin-access/#ssh-to-the-guest-cluster-nodes","title":"SSH to the Guest Cluster nodes","text":"<p>Commands should be run on the supervisor cluster.</p> <p>SSH commands can be run from anywhere with access to the VMs.</p>"},{"location":"tkgs/admin-access/#using-private-keys","title":"Using private keys","text":"<p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh -o jsonpath={.data.\"ssh-privatekey\"}  | base64 -d | tee -a privatekey.key\nchmod 600 /tmp/privatekey.key\nssh -i /tmp/privatekey.key vmware-system-user@&lt;guest_IP&gt;\n</code></pre> The private key can be exported and if not should be deleted when finished.</p>"},{"location":"tkgs/admin-access/#using-the-password","title":"Using the password","text":"<pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster name&gt;-ssh-password -o jsonpath={.data.\"ssh-passwordkey\"} | base64 -d\nssh vmware-system-user@&lt;guest_IP&gt;\n</code></pre>"},{"location":"tkgs/admin-access/#running-commands-without-kubectl-for-debugging","title":"Running commands without kubectl, for debugging","text":"<p><code>crictl</code> mostly mirrors <code>docker</code></p> <pre><code>sudo -i\ncrictl ps # show running containers\ncrictl logs &lt;container ID&gt; # show container logs\ncrtictl exec pod &lt;container ID&gt; /bin/sh # exec into a container\n</code></pre>"},{"location":"tkgs/admin-access/#getting-kubectl-inside-a-workload-cluster-control-plane-node","title":"Getting kubectl inside a workload cluster control plane node","text":"<pre><code>sudo -i\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre>"},{"location":"tkgs/admin-access/#legacy-tkgs-harbor-admin-ui-access","title":"Legacy TKGS Harbor Admin UI Access","text":"<p><code>kubectl</code> commands must be run from inside a supervisor cluster VM.</p> <pre><code>HARBOR_NAMESPACE=$(kubectl get ns | grep registry- | awk '{print $1}')\nHARBOR_POD_ID=$(echo $HARBOR_NAMESPACE | sed 's/.*-//')\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminUsername}' |base64 -d |base64 -d\nkubectl get secret -n $HARBOR_NAMESPACE harbor-$HARBOR_POD_ID-controller-registry -o=jsonpath='{.data.harborAdminPassword}' |base64 -d |base64 -d\n</code></pre>"},{"location":"tkgs/set-default-psa/","title":"Set default PSA on vSphere with Tanzu 1.26+ clusters","text":"<p>From TKR 1.26 and above vSphere with Tanzu changed the default Pod Security Admission (PSA) level to enforced, meaning that each namespace must be labelled to relax the policy. The instructions on this page explain how to create a custom cluster class from the default class which sets the default PSA level to audit.</p>"},{"location":"tkgs/set-default-psa/#warning","title":"Warning","text":"<p>Taken from the vSphere with Tanzu docs.</p> <p>\"Custom ClusterClass is an experimental Kubernetes feature per the upstream Cluster API documentation. Due to the range of customizations available with custom ClusterClass, VMware cannot test or validate all possible customizations. Customers are responsible for testing, validating, and troubleshooting their custom ClusterClass clusters. Customers can open support tickets regarding their custom ClusterClass clusters, however, VMware support is limited to a best effort basis only and cannot guarantee resolution to every issue opened for custom ClusterClass clusters. Customers should be aware of these risks before deploying custom ClusterClass clusters in production environments.\"</p> <p>Given the statement above and the fact that a cluster cannot currently be switched between cluster classes, it is not recommended to use custom cluster classes.</p> <p>The procedure is based on the {vSphere docs](https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-with-tanzu-tkg/GUID-EFE7DB40-8748-42B5-9694-DBC21F9FB76A.html), which you should always reference to check for changes.</p>"},{"location":"tkgs/set-default-psa/#procedure","title":"Procedure","text":""},{"location":"tkgs/set-default-psa/#step-1-copy-the-default-clusterclass","title":"Step 1 - Copy the default ClusterClass","text":"<p>Export the variables to match your environment.</p> <pre><code>export NS=\"ns1\"\nexport CCC_NAME=\"my-cc\"\n</code></pre> <p>Export the default ClusterClass, strip unnecessary fields and update the name. <pre><code>kubectl -n $NS get clusterclass tanzukubernetescluster -o yaml &gt; ccc.yaml\nsed -i '/creationTimestamp:/d' ccc.yaml &amp;&amp; sed -i '/generation:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml &amp;&amp; sed -i '/uid:/d' ccc.yaml &amp;&amp; \\\n sed -i '/resourceVersion:/d' ccc.yaml\nsed -i \"s/  name: tanzukubernetescluster/  name: ${CCC_NAME}/g\" ccc.yaml\n</code></pre></p>"},{"location":"tkgs/set-default-psa/#step-2-modify-the-custom-clusterclass","title":"Step 2 - Modify the custom ClusterClass","text":"<p>It's recommended to manually edit the file to set policy, but automated step are listed below.</p> <ul> <li>Open ccc.yaml in your favourity editor.</li> <li>Search for <code>controlPlaneFilesAdmissionConfigurationk8s126</code> and scroll up to see the <code>AdmissionConfiguration</code> template.</li> <li>Modify the yaml to set your policy by updating the section <code>plugins.0.configuration.defaults</code>. Scrolling up 30 lines will show the K8s 1.25 policy which does not enforce.</li> </ul>"},{"location":"tkgs/set-default-psa/#automated-steps-to-replace-out-the-fields-of-the-k8s-125-policy","title":"Automated steps to replace out the fields of the K8s 1.25 policy.","text":"<p>Use with caution as this was only tested with 1.26.</p> <pre><code>sed -i -E 's/enforce: \"restricted\"/warn: \"restricted\"\\n                      warn-version: \"latest\"/' ccc.yaml\nsed -i -E 's/enforce-version: \"latest\"/audit: \"restricted\"\\n                      audit-version: \"latest\"/' ccc.yaml\n</code></pre>"},{"location":"tkgs/set-default-psa/#step-3-apply-the-custom-clusterclass-to-any-namespace","title":"Step 3 - Apply the custom ClusterClass to any namespace","text":"<p>The ClusterClass to any namespaces where it is needed.</p> <pre><code>export TARGET_NS=\"ns2\"\nsed -i \"s/namespace: .*/namespace: ${TARGET_NS}/g\" ccc.yaml\nkubectl apply -f ccc.yaml\n</code></pre>"},{"location":"tkgs/set-default-psa/#step-4-create-clusters-using-the-custom-clusterclass","title":"Step 4 - Create clusters using the custom ClusterClass","text":"<p>Add the following section to your ClusterClass yamls <pre><code>spec:\n  topology:\n    class: &lt;custom cluster class name&gt;\n</code></pre></p>"},{"location":"tkgs/update-cp-disks/","title":"How to update the control plane disks of a workload cluster on vSphere with Tanzu","text":"<p>The docs explains how to expand the disk of a vSphere with Tanzu control plane VMs in place.</p>"},{"location":"tkgs/update-cp-disks/#this-procedure-is-not-supported-by-vmware-and-must-be-used-with-caution-the-author-cannot-be-held-responsible-for-any-issues","title":"THIS PROCEDURE IS NOT SUPPORTED BY VMWARE AND MUST BE USED WITH CAUTION! THE AUTHOR CANNOT BE HELD RESPONSIBLE FOR ANY ISSUES.","text":""},{"location":"tkgs/update-cp-disks/#running-without-the-update-validation-webhook-is-risky-as-no-control-plane-update-actions-will-be-validated-so-should-be-done-for-the-minimum-amount-of-time","title":"Running without the update validation webhook is risky as no control plane update actions will be validated, so should be done for the minimum amount of time.","text":""},{"location":"tkgs/update-cp-disks/#connect-to-a-supervisor-vm","title":"Connect to a Supervisor VM","text":"<p>SSH into the vCenter as root and enter a shell.</p> <p>Run: <pre><code>/usr/lib/vmware-wcp/decryptK8Pwd.py\n</code></pre></p> <p>SSH into the IP address provided by the script output as the root user, using the password provided.</p>"},{"location":"tkgs/update-cp-disks/#on-a-supervisor-vm","title":"On a supervisor VM","text":"<p>Run: <pre><code>kubectl get ValidatingWebhookConfiguration capi-kubeadm-control-plane-validating-webhook-configuration -o yaml &gt; cp-hook-org.yml\ncp cp-hook-org.yml cp-hook-upd.yml\nkubectl get ValidatingWebhookConfiguration vmware-system-tkg-validating-webhook-configuration -o yaml &gt; tkg-hook-org.yml\ncp tkg-hook-org.yml tkg-hook-upd.yml\n</code></pre></p> <p>Edit <code>cp-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: validation.kubeadmcontrolplane.controlplane.cluster.x-k8s.io</code> (this was the first webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Edit <code>tkg-hook-upd.yml</code> and find the object under <code>webhooks:</code> with <code>name: default.validating.tanzukubernetescluster.run.tanzu.vmware.com</code> (this was the last webhook on my 8.0 update 2 system) then remove <code>- UPDATE</code> line under <code>rules[0].operations</code>.</p> <p>Run: <pre><code>kubectl apply -f cp-hook-upd.yml\nkubectl apply -f tkg-hook-upd.yml\n</code></pre></p>"},{"location":"tkgs/update-cp-disks/#updating-workload-cluster-via-supervisor-manifest","title":"Updating workload cluster via Supervisor manifest","text":"<p>On a standard linux machine with a normal connection to the supervisor</p> <p>Now it is possible to update the control plane disks. Be careful to not make any other changes.</p>"},{"location":"tkgs/update-cp-disks/#updating-class-based-workload-clusers-via-tmc","title":"Updating Class based workload clusers via TMC","text":"<p>Instructions use the Tanzu CLI.</p> <p>Export the existing cluster config to yaml: <pre><code>tanzu management-cluster cluster get &lt;cluster_name&gt; -p &lt;supervisor_namesapce&gt; -m &lt;tmc_supervisor_name&gt; -o yaml &gt; cluster_spec.yaml\n</code></pre></p> <p>Remove the \"meta\" and \"status\" sections.</p> <p>Under spec.topology.variables add the following (update the mount, storageClass and sizes as necessary): <pre><code>spec:\n  topology:\n    variables:\n    - name: controlPlaneVolumes\n      value:\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/containerd\n        name: containerd\n        storageClass: tkgs-storage-policy\n      - capacity:\n          storage: 30G\n        mountPath: /var/lib/kubelet\n        name: kubelet\n        storageClass: tkgs-storage-policy\n</code></pre></p>"},{"location":"tkgs/update-cp-disks/#revert-on-completion","title":"Revert on completion!!!!","text":"<p>Once finished, revert the changes on the supervisor by addding the <code>- UPDATE</code> lines that were removed at the start.</p> <p>You can re-apply the original files by SSHing back to the same supervisor node, which should happen automatically following the connection procedure. </p> <p>Run: <pre><code>kubectl apply -f cp-hook-org.yml\nkubectl apply -f tkg-hook-org.yml\n</code></pre></p> <p>If for any reason you are not able to access the original yaml files you can follow the process in reverse by adding the relevant lines.</p>"}]}